root@DESKTOP-08HJS8G:/mnt/d/Documents/Projects/Thesis/src# python train.py all --reload --dataset-fraction 0.5 --normalization per_sample
ðŸ”§ Environment variables set for TensorFlow/CUDA compatibility
âœ… Import safety check passed - TensorFlow not yet imported
2025-06-07 00:43:20.132183: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2025-06-07 00:43:20.132295: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-06-07 00:43:20.441100: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2025-06-07 00:43:21.363537: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-06-07 00:43:26.932259: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
ðŸ”§ Setting up GPU configuration...
ðŸš€ Setting up GPU-friendly configuration for GTX 1050 (2GB VRAM)...
âœ… CUDA libdevice path set to: /usr/lib/cuda
ðŸ”§ Configuring GPU memory settings...
2025-06-07 00:43:32.682382: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node
Your kernel may have been built without NUMA support.
2025-06-07 00:43:34.922903: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node
Your kernel may have been built without NUMA support.
2025-06-07 00:43:34.923085: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node
Your kernel may have been built without NUMA support.
âœ… Found 1 GPU(s): ['/physical_device:GPU:0']
âœ… Memory growth enabled for /physical_device:GPU:0
ðŸ”§ Configuring TensorFlow memory management...
âœ… Memory growth enabled for /physical_device:GPU:0
âœ… Using float32 precision (optimized for GTX 1050/Pascal)
   This provides better performance than mixed precision on compute capability 6.1
âœ… GPU configuration complete! GTX 1050 optimized with correct CUDA paths.
âœ… GPU configuration successful - using optimized settings
ðŸš€ ENHANCED TRAINING PIPELINE
======================================================================
ðŸ”§ Normalization: per_sample (strategy: separate)
ðŸ“Š Dataset fraction: 50.0%
ðŸ’¾ Memory limit: 3.5GB
ðŸŽ¯ Validation strategy: auto
ðŸ“Š Memory monitoring: Enabled
ðŸ“ˆ Progress bars: Enabled
ðŸ–¥ï¸  GPU optimization: Enabled

ðŸ“Š Initial system status:
ðŸ’¾ Initial memory usage: 0.61GB

ðŸ“¦ Preparing dataset with smart normalization...
ðŸ§  Memory limit applied: 3.5GB
   ðŸ“Š Calculated chunk size: 5,000 samples
   ðŸ“Š Estimated memory per chunk: 0.11GB
ðŸ“ Generating fresh dataset with chunk size: 5,000
ðŸ§  Loading and analyzing EEG data...
ðŸ“Š EEG Dataset Analysis:
   Total samples: 53,760
   Columns: 6017
   Estimated memory: 2467.9 MB
   Feature columns: 6016
   ðŸ“¡ Parsing structured column names...
   ðŸ“¡ Detected from names: 32 channels Ã— 188 timepoints
ðŸ”„ Loading EEG data in chunks...
   Processing EEG chunk 1: samples 0-5,000
   Processing EEG chunk 2: samples 5,000-10,000
   Processing EEG chunk 3: samples 10,000-15,000
   Processing EEG chunk 4: samples 15,000-20,000
   Processing EEG chunk 5: samples 20,000-25,000
   Processing EEG chunk 6: samples 25,000-30,000
   Processing EEG chunk 7: samples 30,000-35,000
   Processing EEG chunk 8: samples 35,000-40,000
   Processing EEG chunk 9: samples 40,000-45,000
   Processing EEG chunk 10: samples 45,000-50,000
   Processing EEG chunk 11: samples 50,000-53,760
   ðŸ”— Combining EEG chunks...
   âœ… EEG data loaded: (53760, 6016)
   ðŸ“Š EEG Statistics:
      Unique labels: [1]
      Data range: [-0.298828, 0.328115]
      Mean: -0.003678, Std: 0.013236
      Memory usage: 2467.5 MB

â¤ï¸  Loading ECG data...
ðŸ“– Loading ECG data from: data/mitbih_train.csv
   ðŸ“Š Raw ECG data shape: (87553, 188)
   ðŸ·ï¸ Original ECG label distribution: [72470  2223  5788   641  6431]
   âœ‚ï¸ ECG samples after filtering (normal heartbeats only): 72470
   ðŸ“Š ECG feature range: [0.000000, 1.000000]
   ðŸ“Š ECG statistics: mean=0.161897, std=0.217563
   âš–ï¸ Balancing ECG to match EEG size: 53,760 samples
   âœ… ECG balanced to: 53,760 samples
   âœ… Final ECG dataset: (53760, 187)
   ðŸ·ï¸ Binary labels: 53760 samples, all labeled as 0 (ECG)
ECG samples loaded: 53760
âœ‚ï¸ Applied dataset_fraction before normalization:
   EEG: 26880 samples | ECG: 26880 samples

ðŸ” Advanced feature compatibility check:
   ECG features: 187
   EEG features: 6016
   ðŸ“Š EEG structure: 32 channels Ã— 188 timepoints
   âš ï¸  Feature dimension mismatch!
   ðŸ§  EEG structure: 32 channels Ã— 188 timepoints
   ðŸ”„ Projecting ECG: 187 â†’ 6016 features
   ðŸ“ˆ Expanding ECG dimensions...
   ðŸ”„ Repeated ECG features 32x + 32 extra
   âœ… After projection - ECG: 6016, EEG: 6016

ðŸ”§ APPLYING NORMALIZATION: per_sample (strategy: separate)

ðŸ”§ Applying per_sample normalization (strategy: separate)

ðŸ” PRE-NORMALIZATION DEBUG (ECG):
   Shape: (26880, 6016)
   Dtype: float64
   Memory: 1.20GB
   Sample size for analysis: 1000
   Overall mean: 0.163178
   Overall std: 0.218215
   Overall range: [0.000000, 1.000000]
   NaN values: 0
   Infinite values: 0
   Zero values: 66,235,318 (41.0%)
   Percentiles:
       0.1%:     0.000000
       1.0%:     0.000000
       5.0%:     0.000000
      25.0%:     0.000000
      50.0%:     0.076253
      75.0%:     0.268443
      95.0%:     0.630390
      99.0%:     0.959128
      99.9%:     1.000000
   IQR outliers (3Ã—IQR): 0 (0.00%)
   Outlier bounds: [-0.805328, 1.073771]
   Per-feature stats (first 10 features):
     Feature  0: mean=  0.9148, std=  0.2134, range=[  0.0000,   1.0000]
     Feature  1: mean=  0.7783, std=  0.1975, range=[  0.0000,   1.0000]
     Feature  2: mean=  0.4039, std=  0.2140, range=[  0.0000,   1.0000]
     Feature  3: mean=  0.1786, std=  0.1665, range=[  0.0000,   1.0000]
     Feature  4: mean=  0.1652, std=  0.1331, range=[  0.0000,   1.0000]
     Feature  5: mean=  0.1812, std=  0.1406, range=[  0.0000,   0.9915]
     Feature  6: mean=  0.1808, std=  0.1567, range=[  0.0000,   1.0000]
     Feature  7: mean=  0.1815, std=  0.1636, range=[  0.0000,   1.0000]
     Feature  8: mean=  0.1842, std=  0.1662, range=[  0.0000,   1.0000]
     Feature  9: mean=  0.1894, std=  0.1671, range=[  0.0000,   1.0000]
   âš ï¸  Data appears to already be min-max normalized!

ðŸ” PRE-NORMALIZATION DEBUG (EEG):
   Shape: (26880, 6016)
   Dtype: float64
   Memory: 1.20GB
   Sample size for analysis: 1000
   Overall mean: -0.001871
   Overall std: 0.010795
   Overall range: [-0.298828, 0.328115]
   NaN values: 0
   Infinite values: 0
   Zero values: 0 (0.0%)
   Percentiles:
       0.1%:    -0.037280
       1.0%:    -0.030401
       5.0%:    -0.023047
      25.0%:    -0.007401
      50.0%:    -0.000371
      75.0%:     0.004799
      95.0%:     0.014059
      99.0%:     0.021468
      99.9%:     0.028777
   IQR outliers (3Ã—IQR): 5,636 (0.00%)
   Outlier bounds: [-0.044004, 0.041401]
   Per-feature stats (first 10 features):
     Feature  0: mean= -0.0078, std=  0.0101, range=[ -0.0320,   0.0079]
     Feature  1: mean= -0.0078, std=  0.0101, range=[ -0.0320,   0.0079]
     Feature  2: mean= -0.0078, std=  0.0101, range=[ -0.0320,   0.0079]
     Feature  3: mean= -0.0078, std=  0.0101, range=[ -0.0320,   0.0079]
     Feature  4: mean= -0.0078, std=  0.0101, range=[ -0.0320,   0.0080]
     Feature  5: mean= -0.0078, std=  0.0101, range=[ -0.0320,   0.0079]
     Feature  6: mean= -0.0078, std=  0.0101, range=[ -0.0320,   0.0080]
     Feature  7: mean= -0.0078, std=  0.0101, range=[ -0.0320,   0.0079]
     Feature  8: mean= -0.0078, std=  0.0101, range=[ -0.0320,   0.0079]
     Feature  9: mean= -0.0078, std=  0.0101, range=[ -0.0320,   0.0079]
ðŸ”§ Using per-sample normalization for both datasets...
ðŸ”§ Applying OPTIMAL per-sample normalization (clip at Â±5.0Ïƒ)...
   ðŸ“Š Based on empirical analysis: eliminates >6Ïƒ values, minimal data loss
   Input shape: (26880, 6016)
   Input range: [0.000000, 1.000000]
   Input stats: mean=0.163178, std=0.218215
   ðŸ“Š Before clipping analysis:
      >6Ïƒ values: 298,540 (0.1846%)
      >5Ïƒ values: 962,445 (0.5952%)
      >5.0Ïƒ values: 962,445 (0.5952%)
   âœ‚ï¸  Applied 5.0Ïƒ clipping:
      Values clipped: 962,445
      Std reduction: 2.6% (from 1.000 to 0.974)
   ðŸ“Š Post-clipping validation:
      >6Ïƒ values: 0 (target: 0)
      >5Ïƒ values: 0 (expected: 0 for 5.0Ïƒ clipping)
      Final range: [-4.826, 5.000]
      Final stats: mean=-0.004565, std=0.973734
   âœ… PERFECT: All extreme values eliminated!
   ðŸŽ¯ Training stability prediction: EXCELLENT
ðŸ”§ Applying OPTIMAL per-sample normalization (clip at Â±4.5Ïƒ)...
   ðŸ“Š Based on empirical analysis: eliminates >6Ïƒ values, minimal data loss
   Input shape: (26880, 6016)
   Input range: [-0.298828, 0.328115]
   Input stats: mean=-0.001871, std=0.010795
   ðŸ“Š Before clipping analysis:
      >6Ïƒ values: 312 (0.0002%)
      >5Ïƒ values: 3,600 (0.0022%)
      >4.5Ïƒ values: 3,619 (0.0022%)
   âœ‚ï¸  Applied 4.5Ïƒ clipping:
      Values clipped: 3,619
      Std reduction: 0.0% (from 1.000 to 1.000)
   ðŸ“Š Post-clipping validation:
      >6Ïƒ values: 0 (target: 0)
      >5Ïƒ values: 0 (expected: 0 for 5.0Ïƒ clipping)
      Final range: [-4.500, 4.500]
      Final stats: mean=-0.000023, std=0.999872
   âœ… PERFECT: All extreme values eliminated!
   ðŸŽ¯ Training stability prediction: EXCELLENT

ðŸ“Š Final normalization results:
   ECG: mean=-0.004565, std=0.973734, range=[-4.826, 5.000]
   EEG: mean=-0.000023, std=0.999872, range=[-4.500, 4.500]
   âœ… No extreme outliers detected - normalization is optimal!

ðŸ” POST-NORMALIZATION DEBUG (ECG):
   Mean: -0.004565
   Std: 0.973734
   Range: [-4.826004, 5.000000]
   âŒ Mean is NOT close to 0 (z-score)
   âŒ Std is NOT close to 1 (z-score)
   âœ… Range is reasonable for z-scores

ðŸ” POST-NORMALIZATION DEBUG (EEG):
   Mean: -0.000023
   Std: 0.999872
   Range: [-4.500000, 4.500000]
   âœ… Mean is close to 0
   âœ… Std is close to 1
   âœ… Range is reasonable for z-scores

ðŸ“¦ Combining normalized datasets...
ðŸ” Signal alignment validation:
   ECG samples: 26880
   EEG samples: 26880
   EEG structure: 32ch Ã— 188tp
   âœ… Signal alignment check completed
ðŸ”€ Shuffling data...
Label distribution after shuffle: [26880 26880]

âœ‚ï¸  Splitting into train/test sets...
âœ… Final dataset summary:
   Train set: (43008, 6016) | Labels: [21504 21504]
   Test set:  (10752, 6016) | Labels: [5376 5376]
   Feature range: [-4.826004, 5.000000]
   Final statistics:
      Mean: -0.002311
      Std: 0.986788
   Memory usage: Train=1.93GB, Test=0.48GB

ðŸ“Ž Caching dataset...
   ðŸ”„ Saving compressed arrays...
   âœ… Cached to: data/preprocessed_dataset.npz
âœ… Dataset ready!
   ðŸ“Š Dataset loaded in 608.5s
   ðŸš€ Train: (43008, 6016), Test: (10752, 6016)
   ðŸ“‰ Using 50.0% of full dataset
   ðŸ“ˆ Total features: 6,016
   ðŸ§  EEG structure: 32 channels Ã— 188 timepoints
   ðŸ’¾ Memory usage: 3.32GB (Î”+2.71GB)
   ðŸ”¢ Data range: [-4.826, 5.000]
   ðŸ“Š Data stats: mean=-0.002311, std=0.986788
   âœ… No extreme outliers detected - normalization looks good!

ðŸš€ Training 6 models with memory leak prevention...
======================================================================

==================== SVM (1/6) ====================
ðŸ“Š Memory before svm: 3.32GB

ðŸ”§ Training SVM...
ðŸ“Š Memory before SVM: CPU=3.32GB, GPU=0.09GB
ðŸ“Š Building SVM model...
ðŸš€ Training SVM...
âœ… SVM training completed in 163.7s

Evaluation for SVM
[[5376    0]
 [   0 5376]]
              precision    recall  f1-score   support

         ECG       1.00      1.00      1.00      5376
         EEG       1.00      1.00      1.00      5376

    accuracy                           1.00     10752
   macro avg       1.00      1.00      1.00     10752
weighted avg       1.00      1.00      1.00     10752

ðŸ’¾ SVM model saved
ðŸ“Š Memory after SVM: CPU=3.34GB, GPU=0.09GB
âœ… svm completed in 173.1s
ðŸ§¹ Cleaning up memory...
ðŸ§¹ Cleaning up TensorFlow memory...
âœ… Memory cleanup completed
ðŸ“Š Memory after cleanup: 3.08GB (Î”-0.24GB)

==================== SIMPLE_CNN (2/6) ====================
ðŸ“Š Memory before simple_cnn: 3.08GB
ðŸ“Š Dataset size: 43,008 samples
ðŸŽ¯ Using validation strategy: split
âš™ï¸  Configuration for SIMPLE_CNN:
   ðŸ“‹ learning_rate: 0.0001
   ðŸ“‹ batch_size: 24
   ðŸ“‹ dropout: 0.5
   ðŸ“‹ filters: [16, 32]
   ðŸ“‹ kernel_sizes: [3, 3]

ðŸš€ Training simple_cnn with simple split validation...
ðŸ“Š Memory before simple_cnn: CPU=3.08GB, GPU=0.09GB
ðŸ”§ Preparing data for simple_cnn...
   Original data shape: (43008, 6016)
   EEG structure: 32 channels Ã— 188 timepoints = 6016 features
   Reshaped to EEG structure: (43008, 188, 32)
   Input shape for model: (188, 32)
2025-06-07 00:56:47.712081: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node
Your kernel may have been built without NUMA support.
2025-06-07 00:56:47.712892: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node
Your kernel may have been built without NUMA support.
2025-06-07 00:56:47.712975: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node
Your kernel may have been built without NUMA support.
2025-06-07 00:56:48.833624: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node
Your kernel may have been built without NUMA support.
2025-06-07 00:56:48.833893: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node
Your kernel may have been built without NUMA support.
2025-06-07 00:56:48.834088: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2022] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.
2025-06-07 00:56:48.834387: I tensorflow/core/common_runtime/gpu/gpu_process_state.cc:236] Using CUDA malloc Async allocator for GPU: 0
2025-06-07 00:56:48.920942: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node
Your kernel may have been built without NUMA support.
2025-06-07 00:56:48.921160: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 1989 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1050, pci bus id: 0000:01:00.0, compute capability: 6.1
ðŸ”§ Creating adam optimizer with lr=0.0001
ðŸ“Š Model summary for simple_cnn:
   Parameters: 95,409
   Input shape: (188, 32)
                                                                                                                 ðŸ§  Small model (95,409 params) - patience reduced to 5                                 | 0/30 [00:00<?, ?epoch/s]
2025-06-07 00:57:10.573255: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:454] Loaded cuDNN version 8907

Epoch 6: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.1.000, Val_Acc=1.000, LR=1.0e-04, GPU=1

Epoch 11: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.1.000, Val_Acc=1.000, LR=5.0e-05, GPU=

Epoch 16: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.1.000, Val_Acc=1.000, LR=2.5e-05, GPU=
                                                                                                                 ðŸ›‘ Early stopping: Plateau detected after 5 epochsepoch] , Loss=0.0000, Acc=1.000, Val_Acc=1.000, LR=1.2e-05, GPU=
   Best val_accuracy: 1.000000 at epoch 15
ðŸ”„ Restoring best weights

Epoch 21: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-06.
âœ… simple_cnn (458.2s):  70%|â–‹| 21/30 [07:38<03:16, 21.82s/epoch] , Loss=0.0000, Acc=1.000, Val_Acc=1.000, LR=1.2e
â±ï¸  Training completed in 462.9s21/30 [07:38<03:00, 20.01s/epoch] , Loss=0.0000, Acc=1.000, Val_Acc=1.000, LR=1.2e
ðŸ“ˆ Training curves saved to: models/simple_cnn_training_20250607_010434.png
ðŸ” Evaluating on test set...

Evaluation for simple_cnn
[[5376    0]
 [   0 5376]]
              precision    recall  f1-score   support

         ECG       1.00      1.00      1.00      5376
         EEG       1.00      1.00      1.00      5376

    accuracy                           1.00     10752
   macro avg       1.00      1.00      1.00     10752
weighted avg       1.00      1.00      1.00     10752

ðŸ“Š Memory after simple_cnn: CPU=4.18GB, GPU=1.50GB
âœ… simple_cnn model saved
âœ… simple_cnn completed in 473.8s
ðŸ§¹ Cleaning up memory...
ðŸ§¹ Cleaning up TensorFlow memory...
âœ… Memory cleanup completed
ðŸ“Š Memory after cleanup: 3.92GB (Î”+0.84GB)

==================== CNN_LSTM (3/6) ====================
ðŸ“Š Memory before cnn_lstm: 3.92GB
ðŸ“Š Dataset size: 43,008 samples
ðŸŽ¯ Using validation strategy: split
âš™ï¸  Configuration for CNN_LSTM:
   ðŸ“‹ learning_rate: 0.0002
   ðŸ“‹ batch_size: 32
   ðŸ“‹ dropout: 0.5
   ðŸ“‹ filters: [16, 32]
   ðŸ“‹ kernel_sizes: [3, 3, 3]
   ðŸ“‹ lstm_units: 16
   ðŸ“‹ l2_regularization: 0.0002

ðŸš€ Training cnn_lstm with simple split validation...
ðŸ“Š Memory before cnn_lstm: CPU=3.92GB, GPU=1.50GB
ðŸ”§ Preparing data for cnn_lstm...
   Original data shape: (43008, 6016)
   EEG structure: 32 channels Ã— 188 timepoints = 6016 features
   Reshaped to EEG structure: (43008, 188, 32)
   Input shape for model: (188, 32)
[DEBUG] CNN-LSTM input_shape received: (188, 32)
[INFO] Assuming format: (timesteps=188, features=32)
ðŸ”§ Creating adam optimizer with lr=0.0002
ðŸ“Š Model summary for cnn_lstm:
   Parameters: 9,649
   Input shape: (188, 32)
                                                                                                                 ðŸ§  Small model (9,649 params) - patience reduced to 5                                  | 0/30 [00:00<?, ?epoch/s]
                                                                                                                 ðŸ›‘ Early stopping: Plateau detected after 5 epochsoch] , Loss=0.0050, Acc=0.999, Val_Acc=1.000, LR=2.0e-04, GPU=1.
   Best val_accuracy: 1.000000 at epoch 15
ðŸ”„ Restoring best weights
âœ… cnn_lstm (609.4s):  70%|â–‹| 21/30 [10:09<04:21, 29.02s/epoch] , Loss=0.0050, Acc=0.999, Val_Acc=1.000, LR=2.0e-0
â±ï¸  Training completed in 615.3s/30 [10:09<04:13, 28.15s/epoch] , Loss=0.0050, Acc=0.999, Val_Acc=1.000, LR=2.0e-0
ðŸ“ˆ Training curves saved to: models/cnn_lstm_training_20250607_011456.png
ðŸ” Evaluating on test set...

Evaluation for cnn_lstm
[[5376    0]
 [   0 5376]]
              precision    recall  f1-score   support

         ECG       1.00      1.00      1.00      5376
         EEG       1.00      1.00      1.00      5376

    accuracy                           1.00     10752
   macro avg       1.00      1.00      1.00     10752
weighted avg       1.00      1.00      1.00     10752

ðŸ“Š Memory after cnn_lstm: CPU=5.29GB, GPU=1.56GB
âœ… cnn_lstm model saved
âœ… cnn_lstm completed in 622.2s
ðŸ§¹ Cleaning up memory...
ðŸ§¹ Cleaning up TensorFlow memory...
âœ… Memory cleanup completed
ðŸ“Š Memory after cleanup: 4.05GB (Î”+0.12GB)

==================== MLP (4/6) ====================
ðŸ“Š Memory before mlp: 4.05GB
ðŸ“Š Dataset size: 43,008 samples
ðŸŽ¯ Using validation strategy: split
âš™ï¸  Configuration for MLP:
   ðŸ“‹ learning_rate: 0.0001
   ðŸ“‹ batch_size: 48
   ðŸ“‹ dropout1: 0.5
   ðŸ“‹ dropout2: 0.3

ðŸš€ Training mlp with simple split validation...
ðŸ“Š Memory before mlp: CPU=4.05GB, GPU=1.56GB
ðŸ”§ Preparing data for mlp...
   Original data shape: (43008, 6016)
   MLP flattened shape: (43008, 6016)
ðŸ”§ Creating adam optimizer with lr=0.0001
ðŸ“Š Model summary for mlp:
   Parameters: 387,201
   Input shape: (6016,)

Epoch 6: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.0, Val_Acc=1.000, LR=1.0e-04, GPU=1.6GB

Epoch 11: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05., Val_Acc=1.000, LR=5.0e-05, GPU=1.6GB

Epoch 16: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05., Val_Acc=1.000, LR=2.5e-05, GPU=1.6GB

Epoch 21: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-06.0, Val_Acc=1.000, LR=1.2e-05, GPU=1.6GB
                                                                                                                 ðŸ›‘ Early stopping: Plateau detected after 8 epochs] , Loss=0.0001, Acc=1.000, Val_Acc=1.000, LR=6.2e-06, GPU=1.6GB
   Best val_accuracy: 1.000000 at epoch 15
ðŸ”„ Restoring best weights
âœ… mlp (209.4s):  80%|â–Š| 24/30 [03:29<00:52,  8.72s/epoch] , Loss=0.0001, Acc=1.000, Val_Acc=1.000, LR=6.2e-06, GP
â±ï¸  Training completed in 215.1s03:29<00:53,  8.99s/epoch] , Loss=0.0001, Acc=1.000, Val_Acc=1.000, LR=6.2e-06, GP
ðŸ“ˆ Training curves saved to: models/mlp_training_20250607_011838.png
ðŸ” Evaluating on test set...

Evaluation for mlp
[[5376    0]
 [   0 5376]]
              precision    recall  f1-score   support

         ECG       1.00      1.00      1.00      5376
         EEG       1.00      1.00      1.00      5376

    accuracy                           1.00     10752
   macro avg       1.00      1.00      1.00     10752
weighted avg       1.00      1.00      1.00     10752

ðŸ“Š Memory after mlp: CPU=5.33GB, GPU=1.56GB
âœ… mlp model saved
âœ… mlp completed in 217.9s
ðŸ§¹ Cleaning up memory...
ðŸ§¹ Cleaning up TensorFlow memory...
âœ… Memory cleanup completed
ðŸ“Š Memory after cleanup: 4.10GB (Î”+0.05GB)

==================== TCN (5/6) ====================
ðŸ“Š Memory before tcn: 4.10GB
ðŸ“Š Dataset size: 43,008 samples
ðŸŽ¯ Using validation strategy: split
âš™ï¸  Configuration for TCN:
   ðŸ“‹ learning_rate: 0.0002
   ðŸ“‹ batch_size: 64
   ðŸ“‹ base_filters: 24
   ðŸ“‹ dropout_rate: 0.1
   ðŸ“‹ dense_units: 32

ðŸš€ Training tcn with simple split validation...
ðŸ“Š Memory before tcn: CPU=4.10GB, GPU=1.56GB
ðŸ”§ Preparing data for tcn...
   Original data shape: (43008, 6016)
   EEG structure: 32 channels Ã— 188 timepoints = 6016 features
   Reshaped to EEG structure: (43008, 188, 32)
   Input shape for model: (188, 32)
ðŸ”§ Creating adam optimizer with lr=0.0002
ðŸ“Š Model summary for tcn:
   Parameters: 8,945
   Input shape: (188, 32)
                                                                                                                 ðŸ§  Small model (8,945 params) - patience reduced to 5                                  | 0/30 [00:00<?, ?epoch/s]
2025-06-07 01:18:51.316940: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:306] gpu_async_0 cuMemAllocAsync failed to allocate 1911660544 bytes: CUDA error: out of memory (CUDA_ERROR_OUT_OF_MEMORY)
 Reported by CUDA: Free memory/Total memory: 402495899/3221094400
2025-06-07 01:18:51.317028: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:311] Stats: Limit:                      2086508955
InUse:                       961137580
MaxInUse:                   2614480284
NumAllocs:                    11390532
MaxAllocSize:               1641127936
Reserved:                            0
PeakReserved:                        0
LargestFreeBlock:                    0

2025-06-07 01:18:51.317112: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:63] Histogram of current allocation: (allocation_size_in_bytes, nb_allocation_of_that_sizes), ...;
2025-06-07 01:18:51.317184: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 4, 90
2025-06-07 01:18:51.317200: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 8, 15
2025-06-07 01:18:51.317241: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 16, 1
2025-06-07 01:18:51.317313: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 64, 12
2025-06-07 01:18:51.317370: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 96, 47
2025-06-07 01:18:51.317450: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 128, 21
2025-06-07 01:18:51.317515: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 256, 12
2025-06-07 01:18:51.317584: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 1024, 3
2025-06-07 01:18:51.317630: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 1028, 1
2025-06-07 01:18:51.317688: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 2304, 6
2025-06-07 01:18:51.317742: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 3072, 9
2025-06-07 01:18:51.317757: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 4096, 3
2025-06-07 01:18:51.317797: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 6144, 12
2025-06-07 01:18:51.317851: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 6912, 7
2025-06-07 01:18:51.317904: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 8192, 6
2025-06-07 01:18:51.317919: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 9216, 4
2025-06-07 01:18:51.317959: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 12288, 3
2025-06-07 01:18:51.318004: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 309656, 1
2025-06-07 01:18:51.318020: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 368640, 3
2025-06-07 01:18:51.318031: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 1155072, 3
2025-06-07 01:18:51.318089: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 1540096, 4
2025-06-07 01:18:51.318105: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 1556480, 1
2025-06-07 01:18:51.318144: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 16780288, 1
2025-06-07 01:18:51.318204: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 931445248, 1
2025-06-07 01:18:51.318465: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:97] CU_MEMPOOL_ATTR_RESERVED_MEM_CURRENT: 2080374784
2025-06-07 01:18:51.318541: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:99] CU_MEMPOOL_ATTR_USED_MEM_CURRENT: 961137580
2025-06-07 01:18:51.318609: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:100] CU_MEMPOOL_ATTR_RESERVED_MEM_HIGH: 2986344448
2025-06-07 01:18:51.318656: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:101] CU_MEMPOOL_ATTR_USED_MEM_HIGH: 2614480284
                                                                                                                  ðŸ›‘ Early stopping: Plateau detected after 5 epochsn:  70%|â–ˆâ–ˆ | 21/30 [07:16<03:01, 20.21s/epoch] , Loss=0.0005, Acc=1.000, Val_Acc=1.000, LR=2.0e-04, GPU=2.2GB
   Best val_accuracy: 1.000000 at epoch 15
ðŸ”„ Restoring best weights
âœ… tcn (436.9s):  70%|â–‹| 21/30 [07:16<03:07, 20.80s/epoch] , Loss=0.0005, Acc=1.000, Val_Acc=1.000, LR=2.0e-04, GP
â±ï¸  Training completed in 440.9s07:16<03:01, 20.21s/epoch] , Loss=0.0005, Acc=1.000, Val_Acc=1.000, LR=2.0e-04, GP
ðŸ“ˆ Training curves saved to: models/tcn_training_20250607_012602.png
ðŸ” Evaluating on test set...

Evaluation for tcn
[[5376    0]
 [   0 5376]]
              precision    recall  f1-score   support

         ECG       1.00      1.00      1.00      5376
         EEG       1.00      1.00      1.00      5376

    accuracy                           1.00     10752
   macro avg       1.00      1.00      1.00     10752
weighted avg       1.00      1.00      1.00     10752

ðŸ“Š Memory after tcn: CPU=5.44GB, GPU=2.19GB
âœ… tcn model saved
âœ… tcn completed in 455.1s
ðŸ§¹ Cleaning up memory...
ðŸ§¹ Cleaning up TensorFlow memory...
âœ… Memory cleanup completed
ðŸ“Š Memory after cleanup: 4.20GB (Î”+0.10GB)

==================== DUAL_BRANCH (6/6) ====================
ðŸ“Š Memory before dual_branch: 4.20GB

ðŸš€ Training Dual-Branch CNN...
ðŸ“Š Memory before dual_branch: CPU=4.20GB, GPU=2.19GB
ðŸ”§ Preparing data for dual_branch...
   Original data shape: (43008, 6016)
   EEG structure: 32 channels Ã— 188 timepoints = 6016 features
   Reshaped to EEG structure: (43008, 188, 32)
   Input shape for model: (188, 32)
ðŸ“Š Dataset size: 43,008 samples
ðŸŽ¯ Using validation strategy: split
âš™ï¸  Configuration for DUAL_BRANCH:
   ðŸ“‹ learning_rate: 0.0001
   ðŸ“‹ batch_size: 16
   ðŸ“‹ dropout1: 0.3
   ðŸ“‹ dropout2: 0.2
   ðŸ“‹ kernel_sizes:
      ecg: [3, 5]
      eeg: [7, 11]
   ðŸ“‹ base_filters: 12
   ðŸ“‹ l2_regularization: 0.0001

ðŸ”§ Creating adam optimizer with lr=0.0001
ðŸ“Š Model summary for dual_branch:
   Parameters: 8,953
   Input shape: (188, 32)
ðŸ“ˆ dual_branch:   0%|                                                                                                               | 0/30 [00:00<?, ?epoch/s] ðŸ§  Small model (8,953 params) - patience reduced to 5
ðŸ“ˆ dual_branch:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ            | 21/30 [15:52<06:40, 44.54s/epoch] , Loss=0.0251, Acc=0.995, Val_Acc=1.000, LR=1.0e-04, GPU=2.2GBðŸ›‘ Early stopping: Plateau detected after 5 epochs
   Best val_accuracy: 1.000000 at epoch 15
ðŸ”„ Restoring best weights
âœ… dual_branch (952.1s):  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹         | 21/30 [15:52<06:48, 45.34s/epoch] , Loss=0.0251, Acc=0.995, Val_Acc=1.000, LR=1.0e-04, GPU=2.2GB
â±ï¸  Training completed in 958.1s
ðŸ“ˆ Training curves saved to: models/dual_branch_training_20250607_014216.png
ðŸ“Š Classification Report:
              precision    recall  f1-score   support

         ECG       1.00      1.00      1.00      5376
         EEG       1.00      1.00      1.00      5376

    accuracy                           1.00     10752
   macro avg       1.00      1.00      1.00     10752
weighted avg       1.00      1.00      1.00     10752

ðŸ“‰ Confusion matrix saved to: models/dual_branch_confusion_matrix.png
ðŸ“Š Memory after dual_branch: CPU=5.21GB, GPU=2.19GB
âœ… Dual-Branch CNN training complete.
âœ… dual_branch completed in 965.2s
ðŸ§¹ Cleaning up memory...
ðŸ§¹ Cleaning up TensorFlow memory...
âœ… Memory cleanup completed
ðŸ“Š Memory after cleanup: 4.22GB (Î”+0.02GB)

ðŸŽ‰ TRAINING PIPELINE COMPLETE!
======================================================================
â±ï¸  Total time: 48.6 minutes
ðŸ“Š Results summary:
   âœ… Successful: 6/6
   âŒ Failed: 0/6
   ðŸš€ Fastest: svm (173.1s)
ðŸ“Š Final memory usage: 4.22GB (total change: +3.61GB)