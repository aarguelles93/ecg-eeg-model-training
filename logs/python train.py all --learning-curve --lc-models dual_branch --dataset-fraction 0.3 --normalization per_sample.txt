(tf-gpu-env) root@DESKTOP-08HJS8G:/mnt/d/Documents/Projects/Thesis/src# python train.py all --learning-curve --lc-models dual_branch --dataset-fraction 0.3 --normalization per_sample
ğŸ”§ Environment variables set for TensorFlow/CUDA compatibility
âœ… Import safety check passed - TensorFlow not yet imported
2025-06-05 20:59:50.292263: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-06-05 21:00:11.797713: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
ğŸ”§ Setting up GPU configuration...
ğŸš€ Setting up GPU-friendly configuration for GTX 1050 (2GB VRAM)...
âœ… CUDA libdevice path set to: /usr/lib/cuda
ğŸ”§ Configuring GPU memory settings...
2025-06-05 21:00:52.595526: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node
Your kernel may have been built without NUMA support.
2025-06-05 21:00:53.503396: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node
Your kernel may have been built without NUMA support.
2025-06-05 21:00:53.503509: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node
Your kernel may have been built without NUMA support.
âœ… Found 1 GPU(s): ['/physical_device:GPU:0']
âœ… Memory growth enabled for /physical_device:GPU:0
ğŸ”§ Configuring TensorFlow memory management...
âœ… Memory growth enabled for /physical_device:GPU:0
âœ… Using float32 precision (optimized for GTX 1050/Pascal)
   This provides better performance than mixed precision on compute capability 6.1
âœ… GPU configuration complete! GTX 1050 optimized with correct CUDA paths.
âœ… GPU configuration successful - using optimized settings
ğŸš€ ENHANCED TRAINING PIPELINE
======================================================================
ğŸ”§ Normalization: per_sample (strategy: separate)
ğŸ“Š Dataset fraction: 30.0%
ğŸ’¾ Memory limit: 3.5GB
ğŸ¯ Validation strategy: auto
ğŸ“Š Memory monitoring: Enabled
ğŸ“ˆ Progress bars: Enabled
ğŸ–¥ï¸  GPU optimization: Enabled

ğŸ“Š Initial system status:
ğŸ’¾ Initial memory usage: 0.44GB

ğŸ“¦ Preparing dataset with smart normalization...
ğŸ§  Memory limit applied: 3.5GB
   ğŸ“Š Calculated chunk size: 5,000 samples
   ğŸ“Š Estimated memory per chunk: 0.11GB
ğŸ” Checking cached dataset: data/preprocessed_dataset.npz
âœ… Cache parameters match - loading cached data
ğŸ“Š Cached data loaded:
   Train: (25804, 6016) | Test: (6452, 6016)
   Range: [-4.826004, 5.000000]
âœ… Dataset ready!
   ğŸ“Š Dataset loaded in 16.9s
   ğŸš€ Train: (25804, 6016), Test: (6452, 6016)
   ğŸ“‰ Using 30.0% of full dataset
   ğŸ“ˆ Total features: 6,016
   ğŸ§  EEG structure: 32 channels Ã— 188 timepoints
   ğŸ’¾ Memory usage: 1.89GB (Î”+1.45GB)
   ğŸ”¢ Data range: [-4.826, 5.000]
   ğŸ“Š Data stats: mean=-0.002264, std=0.987077
   âœ… No extreme outliers detected - normalization looks good!

ğŸ¯ LEARNING CURVE ANALYSIS REQUESTED
============================================================
ğŸ“Š Analyzing models: dual_branch
ğŸ“ˆ Sample fractions: [0.05, 0.1, 0.2, 0.3, 0.5, 0.7, 1.0]
ğŸ”„ CV folds: 3
ğŸš€ LEARNING CURVE ANALYSIS STARTING
============================================================
ğŸ“Š Dataset size: 25,804 training samples
ğŸ” Models to analyze: dual_branch
ğŸ“ˆ Sample fractions: [0.05, 0.1, 0.2, 0.3, 0.5, 0.7, 1.0]
ğŸ”„ Cross-validation folds: 3

ğŸ” Analyzing learning curve for DUAL_BRANCH
============================================================

ğŸ“Š Training on 5.0% of data...
   ğŸ’¾ Memory before: 1.91GB
ğŸ”§ Creating adam optimizer with lr=0.0001
2025-06-05 21:01:39.482150: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8907
   Fold 1: Train=0.965, Val=0.975, Time=160.9s
ğŸ§¹ Cleaning up TensorFlow memory...
âœ… Memory cleanup completed
ğŸ”§ Creating adam optimizer with lr=0.0001
   Fold 2: Train=0.965, Val=0.973, Time=137.8s
ğŸ§¹ Cleaning up TensorFlow memory...
âœ… Memory cleanup completed
ğŸ”§ Creating adam optimizer with lr=0.0001
   Fold 3: Train=0.983, Val=0.973, Time=138.5s
ğŸ§¹ Cleaning up TensorFlow memory...
âœ… Memory cleanup completed
   ğŸ’¾ Memory after: 4.29GB (Î”+2.38GB)
   ğŸ“ˆ Average: Train=0.971Â±0.008, Val=0.974Â±0.001
ğŸ§¹ Cleaning up TensorFlow memory...
âœ… Memory cleanup completed

ğŸ“Š Training on 10.0% of data...
   ğŸ’¾ Memory before: 4.29GB
ğŸ”§ Creating adam optimizer with lr=0.0001
   Fold 1: Train=0.969, Val=0.999, Time=171.0s
ğŸ§¹ Cleaning up TensorFlow memory...
âœ… Memory cleanup completed
ğŸ”§ Creating adam optimizer with lr=0.0001
   Fold 2: Train=0.987, Val=0.999, Time=167.9s
ğŸ§¹ Cleaning up TensorFlow memory...
âœ… Memory cleanup completed
ğŸ”§ Creating adam optimizer with lr=0.0001
   Fold 3: Train=0.981, Val=0.997, Time=168.5s
ğŸ§¹ Cleaning up TensorFlow memory...
âœ… Memory cleanup completed
   ğŸ’¾ Memory after: 4.31GB (Î”+0.02GB)
   ğŸ“ˆ Average: Train=0.979Â±0.007, Val=0.998Â±0.001
ğŸ§¹ Cleaning up TensorFlow memory...
âœ… Memory cleanup completed

ğŸ“Š Training on 20.0% of data...
   ğŸ’¾ Memory before: 4.31GB
ğŸ”§ Creating adam optimizer with lr=0.0001
   Fold 1: Train=0.995, Val=1.000, Time=187.4s
ğŸ§¹ Cleaning up TensorFlow memory...
âœ… Memory cleanup completed
ğŸ”§ Creating adam optimizer with lr=0.0001
   Fold 2: Train=0.992, Val=0.999, Time=200.6s
ğŸ§¹ Cleaning up TensorFlow memory...
âœ… Memory cleanup completed
ğŸ”§ Creating adam optimizer with lr=0.0001
   Fold 3: Train=0.997, Val=1.000, Time=230.9s
ğŸ§¹ Cleaning up TensorFlow memory...
âœ… Memory cleanup completed
   ğŸ’¾ Memory after: 4.33GB (Î”+0.02GB)
   ğŸ“ˆ Average: Train=0.995Â±0.002, Val=1.000Â±0.000
ğŸ§¹ Cleaning up TensorFlow memory...
âœ… Memory cleanup completed

ğŸ“Š Training on 30.0% of data...
   ğŸ’¾ Memory before: 4.33GB
ğŸ”§ Creating adam optimizer with lr=0.0001
   Fold 1: Train=0.997, Val=1.000, Time=296.3s
ğŸ§¹ Cleaning up TensorFlow memory...
âœ… Memory cleanup completed
ğŸ”§ Creating adam optimizer with lr=0.0001
   Fold 2: Train=1.000, Val=1.000, Time=299.1s
ğŸ§¹ Cleaning up TensorFlow memory...
âœ… Memory cleanup completed
ğŸ”§ Creating adam optimizer with lr=0.0001
   Fold 3: Train=0.999, Val=1.000, Time=282.2s
ğŸ§¹ Cleaning up TensorFlow memory...
âœ… Memory cleanup completed
   ğŸ’¾ Memory after: 4.34GB (Î”+0.01GB)
   ğŸ“ˆ Average: Train=0.999Â±0.001, Val=1.000Â±0.000
   ğŸ›‘ Early stopping: Improvement 0.0020 < threshold 0.005
   ğŸ¯ Optimal dataset size found at 30.0%
ğŸ§¹ Cleaning up TensorFlow memory...
âœ… Memory cleanup completed
ğŸ§¹ Memory cleaned up after dual_branch
ğŸ§¹ Cleaning up TensorFlow memory...
âœ… Memory cleanup completed
ğŸ“Š Learning curves saved to: learning_curves/learning_curves_20250605_214229.png

ğŸ¯ LEARNING CURVE ANALYSIS RECOMMENDATIONS
============================================================

ğŸ” DUAL_BRANCH Analysis:
   ğŸ¯ Optimal dataset size: 7,741 samples (30.0%) - Accuracy: 1.000
   âš¡ Efficient dataset size: 1,290 samples (5.0%) - Time: 145.7s
   ğŸ­ Overfitting level: Low (gap: -0.001)