(venv) root@DESKTOP-08HJS8G:/mnt/d/Documents/Projects/Thesis/src# python train.py tcn --dataset-fraction 0.15 --normalization per_sample
ðŸ”§ Environment variables set for TensorFlow/CUDA compatibility
âœ… Import safety check passed - TensorFlow not yet imported
2025-06-06 16:13:57.286853: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-06-06 16:14:05.129047: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
ðŸ”§ Setting up GPU configuration...
ðŸš€ Setting up GPU-friendly configuration for GTX 1050 (2GB VRAM)...
âœ… CUDA libdevice path set to: /usr/lib/cuda
ðŸ”§ Configuring GPU memory settings...
2025-06-06 16:14:17.457377: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node
Your kernel may have been built without NUMA support.
2025-06-06 16:14:23.759350: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node
Your kernel may have been built without NUMA support.
2025-06-06 16:14:23.759516: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node
Your kernel may have been built without NUMA support.
âœ… Found 1 GPU(s): ['/physical_device:GPU:0']
âœ… Memory growth enabled for /physical_device:GPU:0
ðŸ”§ Configuring TensorFlow memory management...
âœ… Memory growth enabled for /physical_device:GPU:0
âœ… Using float32 precision (optimized for GTX 1050/Pascal)
   This provides better performance than mixed precision on compute capability 6.1
âœ… GPU configuration complete! GTX 1050 optimized with correct CUDA paths.
âœ… GPU configuration successful - using optimized settings
ðŸš€ ENHANCED TRAINING PIPELINE
======================================================================
ðŸ”§ Normalization: per_sample (strategy: separate)
ðŸ“Š Dataset fraction: 15.0%
ðŸ’¾ Memory limit: 3.5GB
ðŸŽ¯ Validation strategy: auto
ðŸ“Š Memory monitoring: Enabled
ðŸ“ˆ Progress bars: Enabled
ðŸ–¥ï¸  GPU optimization: Enabled

ðŸ“Š Initial system status:
ðŸ’¾ Initial memory usage: 0.46GB

ðŸ“¦ Preparing dataset with smart normalization...
ðŸ§  Memory limit applied: 3.5GB
   ðŸ“Š Calculated chunk size: 5,000 samples
   ðŸ“Š Estimated memory per chunk: 0.11GB
ðŸ” Checking cached dataset: data/preprocessed_dataset.npz
âœ… Cache parameters match - loading cached data
ðŸ“Š Cached data loaded:
   Train: (86016, 6016) | Test: (21504, 6016)
   Range: [-4.826004, 5.000000]
âœ… Dataset ready!
   ðŸ“Š Dataset loaded in 260.9s
   ðŸš€ Train: (86016, 6016), Test: (21504, 6016)
   ðŸ“‰ Using 15.0% of full dataset
   ðŸ“ˆ Total features: 6,016
   ðŸ§  EEG structure: 32 channels Ã— 188 timepoints
   ðŸ’¾ Memory usage: 5.28GB (Î”+4.82GB)
   ðŸ”¢ Data range: [-4.826, 5.000]
   ðŸ“Š Data stats: mean=-0.002794, std=0.984329
   âœ… No extreme outliers detected - normalization looks good!

ðŸš€ Training 1 models with memory leak prevention...
======================================================================

==================== TCN (1/1) ====================
ðŸ“Š Memory before tcn: 5.28GB
ðŸ“Š Dataset size: 86,016 samples
ðŸŽ¯ Using validation strategy: split
âš™ï¸  Configuration for TCN:
   ðŸ“‹ learning_rate: 0.0002
   ðŸ“‹ batch_size: 64
   ðŸ“‹ base_filters: 24
   ðŸ“‹ dropout_rate: 0.1
   ðŸ“‹ dense_units: 32

ðŸš€ Training tcn with simple split validation...
ðŸ“Š Memory before tcn: CPU=5.28GB, GPU=0.09GB
ðŸ”§ Preparing data for tcn...
   Original data shape: (86016, 6016)
   EEG structure: 32 channels Ã— 188 timepoints = 6016 features
   Reshaped to EEG structure: (86016, 188, 32)
   Input shape for model: (188, 32)
2025-06-06 16:20:51.442266: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1726] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.
2025-06-06 16:20:51.442412: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node
Your kernel may have been built without NUMA support.
2025-06-06 16:20:51.442660: I tensorflow/core/common_runtime/gpu/gpu_process_state.cc:227] Using CUDA malloc Async allocator for GPU: 0
2025-06-06 16:20:51.563619: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 1875 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1050, pci bus id: 0000:01:00.0, compute capability: 6.1
ðŸ”§ Creating adam optimizer with lr=0.0002
ðŸ“Š Model summary for tcn:
   Parameters: 8,945
   Input shape: (188, 32)
2025-06-06 16:21:27.677953: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 1862890496 exceeds 10% of free system memory.
2025-06-06 16:26:04.097976: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 1862890496 exceeds 10% of free system memory.
                                                                                                                                                                                                                                          ðŸ§  Small model (8,945 params) - patience reduced to 5                                                                                                                                                           | 0/30 [00:00<?, ?epoch/s]
2025-06-06 16:27:10.215630: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8907
2025-06-06 16:27:18.402127: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:306] gpu_async_0 cuMemAllocAsync failed to allocate 1504709632 bytes: CUDA error: out of memory (CUDA_ERROR_OUT_OF_MEMORY)
 Reported by CUDA: Free memory/Total memory: 429758875/3221094400
2025-06-06 16:27:18.402185: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:311] Stats: Limit:                      1966971291
InUse:                      1885838620
MaxInUse:                   1904155932
NumAllocs:                         213
MaxAllocSize:               1862890496
Reserved:                            0
PeakReserved:                        0
LargestFreeBlock:                    0

2025-06-06 16:27:18.402285: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:63] Histogram of current allocation: (allocation_size_in_bytes, nb_allocation_of_that_sizes), ...;
2025-06-06 16:27:18.402315: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 4, 54
2025-06-06 16:27:18.402345: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 8, 8
2025-06-06 16:27:18.402373: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 16, 1
2025-06-06 16:27:18.402401: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 96, 42
2025-06-06 16:27:18.402410: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 128, 6
2025-06-06 16:27:18.402416: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 256, 2
2025-06-06 16:27:18.402421: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 1028, 1
2025-06-06 16:27:18.402426: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 2304, 8
2025-06-06 16:27:18.402431: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 3072, 9
2025-06-06 16:27:18.402436: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 6912, 8
2025-06-06 16:27:18.402444: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 9216, 4
2025-06-06 16:27:18.402450: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 619312, 1
2025-06-06 16:27:18.402473: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 1155072, 1
2025-06-06 16:27:18.402482: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 1540096, 1
2025-06-06 16:27:18.402490: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 1556480, 1
2025-06-06 16:27:18.402496: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 17932288, 1
2025-06-06 16:27:18.402519: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 1862890496, 1
2025-06-06 16:27:18.402579: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:97] CU_MEMPOOL_ATTR_RESERVED_MEM_CURRENT: 1946157056
2025-06-06 16:27:18.402608: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:99] CU_MEMPOOL_ATTR_USED_MEM_CURRENT: 1885838620
2025-06-06 16:27:18.402617: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:100] CU_MEMPOOL_ATTR_RESERVED_MEM_HIGH: 3456106496
2025-06-06 16:27:18.402623: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:101] CU_MEMPOOL_ATTR_USED_MEM_HIGH: 1904155932
                                                                                                                                                                                                                                          ðŸ›‘ Early stopping: Plateau detected after 5 epochsâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                     | 21/30 [15:36<06:21, 42.41s/epoch] , Loss=0.0002, Acc=1.000, Val_Acc=1.000, LR=2.0e-04, GPU=2.3GB
   Best val_accuracy: 1.000000 at epoch 15
ðŸ”„ Restoring best weights
âœ… tcn (940.5s):  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                  | 21/30 [15:36<06:41, 44.61s/epoch] , Loss=0.0002, Acc=1.000, Val_Acc=1.000, LR=2.0e-04, GPU=2.3GB
â±ï¸  Training completed in 1258.1sâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                  | 21/30 [15:36<06:21, 42.41s/epoch] , Loss=0.0002, Acc=1.000, Val_Acc=1.000, LR=2.0e-04, GPU=2.3GB
ðŸ“ˆ Training curves saved to: models/tcn_training_20250606_164159.png
ðŸ” Evaluating on test set...
2025-06-06 16:43:55.848903: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:306] gpu_async_0 cuMemAllocAsync failed to allocate 963644416 bytes: CUDA error: out of memory (CUDA_ERROR_OUT_OF_MEMORY)
 Reported by CUDA: Free memory/Total memory: 0/3221094400
2025-06-06 16:43:55.849016: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:311] Stats: Limit:                      1966971291
InUse:                      2607440248
MaxInUse:                   3798645744
NumAllocs:                     8725759
MaxAllocSize:               1911660544
Reserved:                            0
PeakReserved:                        0
LargestFreeBlock:                    0

2025-06-06 16:43:55.849071: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:63] Histogram of current allocation: (allocation_size_in_bytes, nb_allocation_of_that_sizes), ...;
2025-06-06 16:43:55.849101: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 4, 49
2025-06-06 16:43:55.849112: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 8, 12
2025-06-06 16:43:55.849151: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 16, 1
2025-06-06 16:43:55.849160: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 96, 46
2025-06-06 16:43:55.849185: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 128, 6
2025-06-06 16:43:55.849194: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 1028, 1
2025-06-06 16:43:55.970953: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 2304, 6
2025-06-06 16:43:55.971029: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 3072, 7
2025-06-06 16:43:55.971044: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 6912, 6
2025-06-06 16:43:55.971052: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 9216, 3
2025-06-06 16:43:55.971060: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 68816, 1
2025-06-06 16:43:55.971068: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 577536, 2
2025-06-06 16:43:55.971075: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 619312, 1
2025-06-06 16:43:55.971087: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 770048, 1
2025-06-06 16:43:55.971094: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 17354752, 1
2025-06-06 16:43:55.971102: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 206998528, 1
2025-06-06 16:43:55.971133: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 517472256, 1
2025-06-06 16:43:55.971145: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 1862890496, 1
2025-06-06 16:43:55.971183: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:97] CU_MEMPOOL_ATTR_RESERVED_MEM_CURRENT: 2650800128
2025-06-06 16:43:55.971200: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:99] CU_MEMPOOL_ATTR_USED_MEM_CURRENT: 2607440248
2025-06-06 16:43:55.971208: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:100] CU_MEMPOOL_ATTR_RESERVED_MEM_HIGH: 3858759680
2025-06-06 16:43:55.971239: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:101] CU_MEMPOOL_ATTR_USED_MEM_HIGH: 3798645744

Evaluation for tcn
[[10752     0]
 [    0 10752]]
              precision    recall  f1-score   support

         ECG       1.00      1.00      1.00     10752
         EEG       1.00      1.00      1.00     10752

    accuracy                           1.00     21504
   macro avg       1.00      1.00      1.00     21504
weighted avg       1.00      1.00      1.00     21504

ðŸ“Š Memory after tcn: CPU=9.12GB, GPU=2.68GB
âœ… tcn model saved
âœ… tcn completed in 1445.7s
ðŸ§¹ Cleaning up memory...
ðŸ§¹ Cleaning up TensorFlow memory...
âœ… Memory cleanup completed
ðŸ“Š Memory after cleanup: 6.60GB (Î”+1.32GB)

ðŸŽ‰ TRAINING PIPELINE COMPLETE!
======================================================================
â±ï¸  Total time: 24.1 minutes
ðŸ“Š Results summary:
   âœ… Successful: 1/1
   âŒ Failed: 0/1
   ðŸš€ Fastest: tcn (1445.7s)
ðŸ“Š Final memory usage: 6.60GB (total change: +6.14GB)