(venv) root@DESKTOP-08HJS8G:/mnt/d/Documents/Projects/Thesis/src# python train.py tcn --dataset-fraction 0.15 --normalization per_sample
🔧 Environment variables set for TensorFlow/CUDA compatibility
✅ Import safety check passed - TensorFlow not yet imported
2025-06-06 16:13:57.286853: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-06-06 16:14:05.129047: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
🔧 Setting up GPU configuration...
🚀 Setting up GPU-friendly configuration for GTX 1050 (2GB VRAM)...
✅ CUDA libdevice path set to: /usr/lib/cuda
🔧 Configuring GPU memory settings...
2025-06-06 16:14:17.457377: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node
Your kernel may have been built without NUMA support.
2025-06-06 16:14:23.759350: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node
Your kernel may have been built without NUMA support.
2025-06-06 16:14:23.759516: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node
Your kernel may have been built without NUMA support.
✅ Found 1 GPU(s): ['/physical_device:GPU:0']
✅ Memory growth enabled for /physical_device:GPU:0
🔧 Configuring TensorFlow memory management...
✅ Memory growth enabled for /physical_device:GPU:0
✅ Using float32 precision (optimized for GTX 1050/Pascal)
   This provides better performance than mixed precision on compute capability 6.1
✅ GPU configuration complete! GTX 1050 optimized with correct CUDA paths.
✅ GPU configuration successful - using optimized settings
🚀 ENHANCED TRAINING PIPELINE
======================================================================
🔧 Normalization: per_sample (strategy: separate)
📊 Dataset fraction: 15.0%
💾 Memory limit: 3.5GB
🎯 Validation strategy: auto
📊 Memory monitoring: Enabled
📈 Progress bars: Enabled
🖥️  GPU optimization: Enabled

📊 Initial system status:
💾 Initial memory usage: 0.46GB

📦 Preparing dataset with smart normalization...
🧠 Memory limit applied: 3.5GB
   📊 Calculated chunk size: 5,000 samples
   📊 Estimated memory per chunk: 0.11GB
🔍 Checking cached dataset: data/preprocessed_dataset.npz
✅ Cache parameters match - loading cached data
📊 Cached data loaded:
   Train: (86016, 6016) | Test: (21504, 6016)
   Range: [-4.826004, 5.000000]
✅ Dataset ready!
   📊 Dataset loaded in 260.9s
   🚀 Train: (86016, 6016), Test: (21504, 6016)
   📉 Using 15.0% of full dataset
   📈 Total features: 6,016
   🧠 EEG structure: 32 channels × 188 timepoints
   💾 Memory usage: 5.28GB (Δ+4.82GB)
   🔢 Data range: [-4.826, 5.000]
   📊 Data stats: mean=-0.002794, std=0.984329
   ✅ No extreme outliers detected - normalization looks good!

🚀 Training 1 models with memory leak prevention...
======================================================================

==================== TCN (1/1) ====================
📊 Memory before tcn: 5.28GB
📊 Dataset size: 86,016 samples
🎯 Using validation strategy: split
⚙️  Configuration for TCN:
   📋 learning_rate: 0.0002
   📋 batch_size: 64
   📋 base_filters: 24
   📋 dropout_rate: 0.1
   📋 dense_units: 32

🚀 Training tcn with simple split validation...
📊 Memory before tcn: CPU=5.28GB, GPU=0.09GB
🔧 Preparing data for tcn...
   Original data shape: (86016, 6016)
   EEG structure: 32 channels × 188 timepoints = 6016 features
   Reshaped to EEG structure: (86016, 188, 32)
   Input shape for model: (188, 32)
2025-06-06 16:20:51.442266: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1726] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.
2025-06-06 16:20:51.442412: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node
Your kernel may have been built without NUMA support.
2025-06-06 16:20:51.442660: I tensorflow/core/common_runtime/gpu/gpu_process_state.cc:227] Using CUDA malloc Async allocator for GPU: 0
2025-06-06 16:20:51.563619: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 1875 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1050, pci bus id: 0000:01:00.0, compute capability: 6.1
🔧 Creating adam optimizer with lr=0.0002
📊 Model summary for tcn:
   Parameters: 8,945
   Input shape: (188, 32)
2025-06-06 16:21:27.677953: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 1862890496 exceeds 10% of free system memory.
2025-06-06 16:26:04.097976: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 1862890496 exceeds 10% of free system memory.
                                                                                                                                                                                                                                          🧠 Small model (8,945 params) - patience reduced to 5                                                                                                                                                           | 0/30 [00:00<?, ?epoch/s]
2025-06-06 16:27:10.215630: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8907
2025-06-06 16:27:18.402127: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:306] gpu_async_0 cuMemAllocAsync failed to allocate 1504709632 bytes: CUDA error: out of memory (CUDA_ERROR_OUT_OF_MEMORY)
 Reported by CUDA: Free memory/Total memory: 429758875/3221094400
2025-06-06 16:27:18.402185: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:311] Stats: Limit:                      1966971291
InUse:                      1885838620
MaxInUse:                   1904155932
NumAllocs:                         213
MaxAllocSize:               1862890496
Reserved:                            0
PeakReserved:                        0
LargestFreeBlock:                    0

2025-06-06 16:27:18.402285: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:63] Histogram of current allocation: (allocation_size_in_bytes, nb_allocation_of_that_sizes), ...;
2025-06-06 16:27:18.402315: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 4, 54
2025-06-06 16:27:18.402345: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 8, 8
2025-06-06 16:27:18.402373: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 16, 1
2025-06-06 16:27:18.402401: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 96, 42
2025-06-06 16:27:18.402410: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 128, 6
2025-06-06 16:27:18.402416: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 256, 2
2025-06-06 16:27:18.402421: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 1028, 1
2025-06-06 16:27:18.402426: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 2304, 8
2025-06-06 16:27:18.402431: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 3072, 9
2025-06-06 16:27:18.402436: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 6912, 8
2025-06-06 16:27:18.402444: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 9216, 4
2025-06-06 16:27:18.402450: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 619312, 1
2025-06-06 16:27:18.402473: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 1155072, 1
2025-06-06 16:27:18.402482: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 1540096, 1
2025-06-06 16:27:18.402490: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 1556480, 1
2025-06-06 16:27:18.402496: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 17932288, 1
2025-06-06 16:27:18.402519: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 1862890496, 1
2025-06-06 16:27:18.402579: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:97] CU_MEMPOOL_ATTR_RESERVED_MEM_CURRENT: 1946157056
2025-06-06 16:27:18.402608: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:99] CU_MEMPOOL_ATTR_USED_MEM_CURRENT: 1885838620
2025-06-06 16:27:18.402617: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:100] CU_MEMPOOL_ATTR_RESERVED_MEM_HIGH: 3456106496
2025-06-06 16:27:18.402623: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:101] CU_MEMPOOL_ATTR_USED_MEM_HIGH: 1904155932
                                                                                                                                                                                                                                          🛑 Early stopping: Plateau detected after 5 epochs█████████████████████████████████████████████████▊                                     | 21/30 [15:36<06:21, 42.41s/epoch] , Loss=0.0002, Acc=1.000, Val_Acc=1.000, LR=2.0e-04, GPU=2.3GB
   Best val_accuracy: 1.000000 at epoch 15
🔄 Restoring best weights
✅ tcn (940.5s):  70%|████████████████████████████████████████████████████████████████████████████████▌                                  | 21/30 [15:36<06:41, 44.61s/epoch] , Loss=0.0002, Acc=1.000, Val_Acc=1.000, LR=2.0e-04, GPU=2.3GB
⏱️  Training completed in 1258.1s█████████████████████████████████████████████████████████████████████▌                                  | 21/30 [15:36<06:21, 42.41s/epoch] , Loss=0.0002, Acc=1.000, Val_Acc=1.000, LR=2.0e-04, GPU=2.3GB
📈 Training curves saved to: models/tcn_training_20250606_164159.png
🔍 Evaluating on test set...
2025-06-06 16:43:55.848903: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:306] gpu_async_0 cuMemAllocAsync failed to allocate 963644416 bytes: CUDA error: out of memory (CUDA_ERROR_OUT_OF_MEMORY)
 Reported by CUDA: Free memory/Total memory: 0/3221094400
2025-06-06 16:43:55.849016: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:311] Stats: Limit:                      1966971291
InUse:                      2607440248
MaxInUse:                   3798645744
NumAllocs:                     8725759
MaxAllocSize:               1911660544
Reserved:                            0
PeakReserved:                        0
LargestFreeBlock:                    0

2025-06-06 16:43:55.849071: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:63] Histogram of current allocation: (allocation_size_in_bytes, nb_allocation_of_that_sizes), ...;
2025-06-06 16:43:55.849101: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 4, 49
2025-06-06 16:43:55.849112: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 8, 12
2025-06-06 16:43:55.849151: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 16, 1
2025-06-06 16:43:55.849160: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 96, 46
2025-06-06 16:43:55.849185: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 128, 6
2025-06-06 16:43:55.849194: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 1028, 1
2025-06-06 16:43:55.970953: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 2304, 6
2025-06-06 16:43:55.971029: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 3072, 7
2025-06-06 16:43:55.971044: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 6912, 6
2025-06-06 16:43:55.971052: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 9216, 3
2025-06-06 16:43:55.971060: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 68816, 1
2025-06-06 16:43:55.971068: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 577536, 2
2025-06-06 16:43:55.971075: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 619312, 1
2025-06-06 16:43:55.971087: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 770048, 1
2025-06-06 16:43:55.971094: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 17354752, 1
2025-06-06 16:43:55.971102: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 206998528, 1
2025-06-06 16:43:55.971133: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 517472256, 1
2025-06-06 16:43:55.971145: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 1862890496, 1
2025-06-06 16:43:55.971183: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:97] CU_MEMPOOL_ATTR_RESERVED_MEM_CURRENT: 2650800128
2025-06-06 16:43:55.971200: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:99] CU_MEMPOOL_ATTR_USED_MEM_CURRENT: 2607440248
2025-06-06 16:43:55.971208: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:100] CU_MEMPOOL_ATTR_RESERVED_MEM_HIGH: 3858759680
2025-06-06 16:43:55.971239: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:101] CU_MEMPOOL_ATTR_USED_MEM_HIGH: 3798645744

Evaluation for tcn
[[10752     0]
 [    0 10752]]
              precision    recall  f1-score   support

         ECG       1.00      1.00      1.00     10752
         EEG       1.00      1.00      1.00     10752

    accuracy                           1.00     21504
   macro avg       1.00      1.00      1.00     21504
weighted avg       1.00      1.00      1.00     21504

📊 Memory after tcn: CPU=9.12GB, GPU=2.68GB
✅ tcn model saved
✅ tcn completed in 1445.7s
🧹 Cleaning up memory...
🧹 Cleaning up TensorFlow memory...
✅ Memory cleanup completed
📊 Memory after cleanup: 6.60GB (Δ+1.32GB)

🎉 TRAINING PIPELINE COMPLETE!
======================================================================
⏱️  Total time: 24.1 minutes
📊 Results summary:
   ✅ Successful: 1/1
   ❌ Failed: 0/1
   🚀 Fastest: tcn (1445.7s)
📊 Final memory usage: 6.60GB (total change: +6.14GB)