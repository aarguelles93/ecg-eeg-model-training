(tf-gpu-env) root@DESKTOP-08HJS8G:/mnt/d/Documents/Projects/Thesis/src# python train.py all --learning-curve --lc-models mlp simple_cnn svm --dataset-fraction 0.3 --normalization per_sample
ðŸ”§ Environment variables set for TensorFlow/CUDA compatibility
âœ… Import safety check passed - TensorFlow not yet imported
2025-06-05 23:10:12.728306: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-06-05 23:10:21.599846: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
ðŸ”§ Setting up GPU configuration...
ðŸš€ Setting up GPU-friendly configuration for GTX 1050 (2GB VRAM)...
âœ… CUDA libdevice path set to: /usr/lib/cuda
ðŸ”§ Configuring GPU memory settings...
2025-06-05 23:10:43.347347: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node
Your kernel may have been built without NUMA support.
2025-06-05 23:10:44.322248: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node
Your kernel may have been built without NUMA support.
2025-06-05 23:10:44.322419: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node
Your kernel may have been built without NUMA support.
âœ… Found 1 GPU(s): ['/physical_device:GPU:0']
âœ… Memory growth enabled for /physical_device:GPU:0
ðŸ”§ Configuring TensorFlow memory management...
âœ… Memory growth enabled for /physical_device:GPU:0
âœ… Using float32 precision (optimized for GTX 1050/Pascal)
   This provides better performance than mixed precision on compute capability 6.1
âœ… GPU configuration complete! GTX 1050 optimized with correct CUDA paths.
âœ… GPU configuration successful - using optimized settings
ðŸš€ ENHANCED TRAINING PIPELINE
======================================================================
ðŸ”§ Normalization: per_sample (strategy: separate)
ðŸ“Š Dataset fraction: 30.0%
ðŸ’¾ Memory limit: 3.5GB
ðŸŽ¯ Validation strategy: auto
ðŸ“Š Memory monitoring: Enabled
ðŸ“ˆ Progress bars: Enabled
ðŸ–¥ï¸  GPU optimization: Enabled

ðŸ“Š Initial system status:
ðŸ’¾ Initial memory usage: 0.44GB

ðŸ“¦ Preparing dataset with smart normalization...
ðŸ§  Memory limit applied: 3.5GB
   ðŸ“Š Calculated chunk size: 5,000 samples
   ðŸ“Š Estimated memory per chunk: 0.11GB
ðŸ” Checking cached dataset: data/preprocessed_dataset.npz
âœ… Cache parameters match - loading cached data
ðŸ“Š Cached data loaded:
   Train: (25804, 6016) | Test: (6452, 6016)
   Range: [-4.826004, 5.000000]
âœ… Dataset ready!
   ðŸ“Š Dataset loaded in 16.6s
   ðŸš€ Train: (25804, 6016), Test: (6452, 6016)
   ðŸ“‰ Using 30.0% of full dataset
   ðŸ“ˆ Total features: 6,016
   ðŸ§  EEG structure: 32 channels Ã— 188 timepoints
   ðŸ’¾ Memory usage: 1.89GB (Î”+1.45GB)
   ðŸ”¢ Data range: [-4.826, 5.000]
   ðŸ“Š Data stats: mean=-0.002264, std=0.987077
   âœ… No extreme outliers detected - normalization looks good!

ðŸŽ¯ LEARNING CURVE ANALYSIS REQUESTED
============================================================
ðŸ“Š Analyzing models: mlp, simple_cnn, svm
ðŸ“ˆ Sample fractions: [0.05, 0.1, 0.2, 0.3, 0.5, 0.7, 1.0]
ðŸ”„ CV folds: 3
ðŸš€ LEARNING CURVE ANALYSIS STARTING
============================================================
ðŸ“Š Dataset size: 25,804 training samples
ðŸ” Models to analyze: mlp, simple_cnn, svm
ðŸ“ˆ Sample fractions: [0.05, 0.1, 0.2, 0.3, 0.5, 0.7, 1.0]
ðŸ”„ Cross-validation folds: 3

ðŸ” Analyzing learning curve for MLP
============================================================

ðŸ“Š Training on 5.0% of data...
   ðŸ’¾ Memory before: 1.91GB
2025-06-05 23:11:06.303649: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node
Your kernel may have been built without NUMA support.
2025-06-05 23:11:06.303787: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node
Your kernel may have been built without NUMA support.
2025-06-05 23:11:06.303867: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node
Your kernel may have been built without NUMA support.
2025-06-05 23:11:08.569331: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node
Your kernel may have been built without NUMA support.
2025-06-05 23:11:08.569513: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node
Your kernel may have been built without NUMA support.
2025-06-05 23:11:08.569552: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1726] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.
2025-06-05 23:11:08.569667: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node
Your kernel may have been built without NUMA support.
2025-06-05 23:11:08.569718: I tensorflow/core/common_runtime/gpu/gpu_process_state.cc:227] Using CUDA malloc Async allocator for GPU: 0
2025-06-05 23:11:08.650192: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 1875 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1050, pci bus id: 0000:01:00.0, compute capability: 6.1
ðŸ”§ Creating adam optimizer with lr=0.0001
   Fold 1: Train=0.993, Val=1.000, Time=18.7s
ðŸ§¹ Cleaning up TensorFlow memory...
âœ… Memory cleanup completed
ðŸ”§ Creating adam optimizer with lr=0.0001
   Fold 2: Train=0.994, Val=1.000, Time=14.0s
ðŸ§¹ Cleaning up TensorFlow memory...
âœ… Memory cleanup completed
ðŸ”§ Creating adam optimizer with lr=0.0001
   Fold 3: Train=0.991, Val=1.000, Time=13.1s
ðŸ§¹ Cleaning up TensorFlow memory...
âœ… Memory cleanup completed
   ðŸ’¾ Memory after: 4.15GB (Î”+2.25GB)
   ðŸ“ˆ Average: Train=0.993Â±0.001, Val=1.000Â±0.000
ðŸ§¹ Cleaning up TensorFlow memory...
âœ… Memory cleanup completed

ðŸ“Š Training on 10.0% of data...
   ðŸ’¾ Memory before: 4.15GB
ðŸ”§ Creating adam optimizer with lr=0.0001
   Fold 1: Train=0.999, Val=0.999, Time=9.7s
ðŸ§¹ Cleaning up TensorFlow memory...
âœ… Memory cleanup completed
ðŸ”§ Creating adam optimizer with lr=0.0001
   Fold 2: Train=0.999, Val=0.999, Time=9.0s
ðŸ§¹ Cleaning up TensorFlow memory...
âœ… Memory cleanup completed
ðŸ”§ Creating adam optimizer with lr=0.0001
   Fold 3: Train=0.999, Val=0.999, Time=16.1s
ðŸ§¹ Cleaning up TensorFlow memory...
âœ… Memory cleanup completed
   ðŸ’¾ Memory after: 4.16GB (Î”+0.01GB)
   ðŸ“ˆ Average: Train=0.999Â±0.000, Val=0.999Â±0.000
ðŸ§¹ Cleaning up TensorFlow memory...
âœ… Memory cleanup completed

ðŸ“Š Training on 20.0% of data...
   ðŸ’¾ Memory before: 4.16GB
ðŸ”§ Creating adam optimizer with lr=0.0001
   Fold 1: Train=0.999, Val=1.000, Time=12.9s
ðŸ§¹ Cleaning up TensorFlow memory...
âœ… Memory cleanup completed
ðŸ”§ Creating adam optimizer with lr=0.0001
   Fold 2: Train=0.999, Val=1.000, Time=12.6s
ðŸ§¹ Cleaning up TensorFlow memory...
âœ… Memory cleanup completed
ðŸ”§ Creating adam optimizer with lr=0.0001
   Fold 3: Train=0.999, Val=1.000, Time=10.7s
ðŸ§¹ Cleaning up TensorFlow memory...
âœ… Memory cleanup completed
   ðŸ’¾ Memory after: 4.17GB (Î”+0.01GB)
   ðŸ“ˆ Average: Train=0.999Â±0.000, Val=1.000Â±0.000
   ðŸ›‘ Early stopping: Improvement 0.0003 < threshold 0.005
   ðŸŽ¯ Optimal dataset size found at 20.0%
ðŸ§¹ Cleaning up TensorFlow memory...
âœ… Memory cleanup completed
ðŸ§¹ Memory cleaned up after mlp
ðŸ§¹ Cleaning up TensorFlow memory...
âœ… Memory cleanup completed

ðŸ” Analyzing learning curve for SIMPLE_CNN
============================================================

ðŸ“Š Training on 5.0% of data...
   ðŸ’¾ Memory before: 3.01GB
ðŸ”§ Creating adam optimizer with lr=0.0001
2025-06-05 23:13:15.956974: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8907
   Fold 1: Train=1.000, Val=0.999, Time=45.8s
ðŸ§¹ Cleaning up TensorFlow memory...
âœ… Memory cleanup completed
ðŸ”§ Creating adam optimizer with lr=0.0001
   Fold 2: Train=1.000, Val=0.999, Time=48.0s
ðŸ§¹ Cleaning up TensorFlow memory...
âœ… Memory cleanup completed
ðŸ”§ Creating adam optimizer with lr=0.0001
   Fold 3: Train=1.000, Val=1.000, Time=26.2s
ðŸ§¹ Cleaning up TensorFlow memory...
âœ… Memory cleanup completed
   ðŸ’¾ Memory after: 4.32GB (Î”+1.30GB)
   ðŸ“ˆ Average: Train=1.000Â±0.000, Val=0.999Â±0.000
ðŸ§¹ Cleaning up TensorFlow memory...
âœ… Memory cleanup completed

ðŸ“Š Training on 10.0% of data...
   ðŸ’¾ Memory before: 4.32GB
ðŸ”§ Creating adam optimizer with lr=0.0001
   Fold 1: Train=1.000, Val=1.000, Time=29.9s
ðŸ§¹ Cleaning up TensorFlow memory...
âœ… Memory cleanup completed
ðŸ”§ Creating adam optimizer with lr=0.0001
   Fold 2: Train=1.000, Val=1.000, Time=55.6s
ðŸ§¹ Cleaning up TensorFlow memory...
âœ… Memory cleanup completed
ðŸ”§ Creating adam optimizer with lr=0.0001
   Fold 3: Train=1.000, Val=1.000, Time=42.6s
ðŸ§¹ Cleaning up TensorFlow memory...
âœ… Memory cleanup completed
   ðŸ’¾ Memory after: 4.33GB (Î”+0.01GB)
   ðŸ“ˆ Average: Train=1.000Â±0.000, Val=1.000Â±0.000
ðŸ§¹ Cleaning up TensorFlow memory...
âœ… Memory cleanup completed

ðŸ“Š Training on 20.0% of data...
   ðŸ’¾ Memory before: 4.33GB
ðŸ”§ Creating adam optimizer with lr=0.0001
   Fold 1: Train=1.000, Val=1.000, Time=63.5s
ðŸ§¹ Cleaning up TensorFlow memory...
âœ… Memory cleanup completed
ðŸ”§ Creating adam optimizer with lr=0.0001
   Fold 2: Train=0.999, Val=1.000, Time=73.4s
ðŸ§¹ Cleaning up TensorFlow memory...
âœ… Memory cleanup completed
ðŸ”§ Creating adam optimizer with lr=0.0001
   Fold 3: Train=1.000, Val=1.000, Time=38.8s
ðŸ§¹ Cleaning up TensorFlow memory...
âœ… Memory cleanup completed
   ðŸ’¾ Memory after: 4.33GB (Î”+0.00GB)
   ðŸ“ˆ Average: Train=1.000Â±0.000, Val=1.000Â±0.000
   ðŸ›‘ Early stopping: Improvement 0.0006 < threshold 0.005
   ðŸŽ¯ Optimal dataset size found at 20.0%
ðŸ§¹ Cleaning up TensorFlow memory...
âœ… Memory cleanup completed
ðŸ§¹ Memory cleaned up after simple_cnn
ðŸ§¹ Cleaning up TensorFlow memory...
âœ… Memory cleanup completed

ðŸ” Analyzing learning curve for SVM
============================================================

ðŸ“Š Training on 5.0% of data...
   ðŸ’¾ Memory before: 3.18GB
   Fold 1: Train=1.000, Val=1.000, Time=5.8s
ðŸ§¹ Cleaning up TensorFlow memory...
âœ… Memory cleanup completed
   Fold 2: Train=1.000, Val=1.000, Time=5.8s
ðŸ§¹ Cleaning up TensorFlow memory...
âœ… Memory cleanup completed
   Fold 3: Train=1.000, Val=1.000, Time=4.8s
ðŸ§¹ Cleaning up TensorFlow memory...
âœ… Memory cleanup completed
   ðŸ’¾ Memory after: 4.33GB (Î”+1.16GB)
   ðŸ“ˆ Average: Train=1.000Â±0.000, Val=1.000Â±0.000
ðŸ§¹ Cleaning up TensorFlow memory...
âœ… Memory cleanup completed

ðŸ“Š Training on 10.0% of data...
   ðŸ’¾ Memory before: 4.33GB
   Fold 1: Train=1.000, Val=1.000, Time=8.4s
ðŸ§¹ Cleaning up TensorFlow memory...
âœ… Memory cleanup completed
   Fold 2: Train=1.000, Val=1.000, Time=7.1s
ðŸ§¹ Cleaning up TensorFlow memory...
âœ… Memory cleanup completed
   Fold 3: Train=1.000, Val=1.000, Time=6.3s
ðŸ§¹ Cleaning up TensorFlow memory...
âœ… Memory cleanup completed
   ðŸ’¾ Memory after: 4.33GB (Î”+0.00GB)
   ðŸ“ˆ Average: Train=1.000Â±0.000, Val=1.000Â±0.000
ðŸ§¹ Cleaning up TensorFlow memory...
âœ… Memory cleanup completed

ðŸ“Š Training on 20.0% of data...
   ðŸ’¾ Memory before: 4.33GB
   Fold 1: Train=1.000, Val=1.000, Time=13.4s
ðŸ§¹ Cleaning up TensorFlow memory...
âœ… Memory cleanup completed
   Fold 2: Train=1.000, Val=1.000, Time=12.3s
ðŸ§¹ Cleaning up TensorFlow memory...
âœ… Memory cleanup completed
   Fold 3: Train=1.000, Val=1.000, Time=12.1s
ðŸ§¹ Cleaning up TensorFlow memory...
âœ… Memory cleanup completed
   ðŸ’¾ Memory after: 4.33GB (Î”+0.00GB)
   ðŸ“ˆ Average: Train=1.000Â±0.000, Val=1.000Â±0.000
   ðŸ›‘ Early stopping: Improvement 0.0003 < threshold 0.005
   ðŸŽ¯ Optimal dataset size found at 20.0%
ðŸ§¹ Cleaning up TensorFlow memory...
âœ… Memory cleanup completed
ðŸ§¹ Memory cleaned up after svm
ðŸ§¹ Cleaning up TensorFlow memory...
âœ… Memory cleanup completed
ðŸ“Š Learning curves saved to: learning_curves/learning_curves_20250605_232206.png

ðŸŽ¯ LEARNING CURVE ANALYSIS RECOMMENDATIONS
============================================================

ðŸ” MLP Analysis:
   ðŸŽ¯ Optimal dataset size: 5,160 samples (20.0%) - Accuracy: 1.000
   âš¡ Efficient dataset size: 2,580 samples (10.0%) - Time: 11.6s
   ðŸŽ­ Overfitting level: Low (gap: -0.001)
   ðŸ’¡ Recommendation: âœ… EXCELLENT! Model performs well with only 20.0% of data

ðŸ” SIMPLE_CNN Analysis:
   ðŸŽ¯ Optimal dataset size: 2,580 samples (10.0%) - Accuracy: 1.000
   âš¡ Efficient dataset size: 1,290 samples (5.0%) - Time: 40.0s
   ðŸŽ­ Overfitting level: Low (gap: -0.000)
   ðŸ’¡ Recommendation: âœ… EXCELLENT! Model performs well with only 10.0% of data

ðŸ” SVM Analysis:
   ðŸŽ¯ Optimal dataset size: 5,160 samples (20.0%) - Accuracy: 1.000
   âš¡ Efficient dataset size: 1,290 samples (5.0%) - Time: 5.5s
   ðŸŽ­ Overfitting level: Low (gap: 0.000)
   ðŸ’¡ Recommendation: âœ… EXCELLENT! Model performs well with only 20.0% of data

ðŸ† OVERALL RECOMMENDATIONS:
   ðŸ¥‡ Top performing models:
      1. svm: 1.000 accuracy with 5,160 samples
      2. simple_cnn: 1.000 accuracy with 2,580 samples
      3. mlp: 1.000 accuracy with 5,160 samples
   âš¡ Most efficient model: simple_cnn - Good performance with only 1,290 samples
ðŸ’¾ Detailed results saved to: learning_curves/learning_curve_results_20250605_232210.json
ðŸ“Š Summary CSV saved to: learning_curves/learning_curve_summary_20250605_232210.csv
ðŸ’¡ Recommendations saved to: learning_curves/recommendations_20250605_232210.json
ðŸ§¹ Cleaning up memory after learning curve analysis...
ðŸ§¹ Cleaning up TensorFlow memory...
âœ… Memory cleanup completed

ðŸ’¡ LEARNING CURVE INSIGHTS:
   ðŸš€ mlp: Can achieve 1.000 accuracy
      with only 2,580 samples (10.0%)
      Potential time savings: ~90.0%
   ðŸš€ simple_cnn: Can achieve 1.000 accuracy
      with only 1,290 samples (5.0%)
      Potential time savings: ~95.0%
   ðŸš€ svm: Can achieve 1.000 accuracy
      with only 1,290 samples (5.0%)
      Potential time savings: ~95.0%