(tf-gpu-env) root@DESKTOP-08HJS8G:/mnt/d/Documents/Projects/Thesis/src# python train.py cnn_lstm --dataset-fraction 0.15 --normalization per_sample
ðŸ”§ Environment variables set for TensorFlow/CUDA compatibility
âœ… Import safety check passed - TensorFlow not yet imported
2025-06-06 08:12:08.368927: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-06-06 08:12:16.242324: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
ðŸ”§ Setting up GPU configuration...
ðŸš€ Setting up GPU-friendly configuration for GTX 1050 (2GB VRAM)...
âœ… CUDA libdevice path set to: /usr/lib/cuda
ðŸ”§ Configuring GPU memory settings...
2025-06-06 08:12:42.687758: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node
Your kernel may have been built without NUMA support.
âœ… Found 1 GPU(s): ['/physical_device:GPU:0']
âœ… Memory growth enabled for /physical_device:GPU:0
ðŸ”§ Configuring TensorFlow memory management...
âœ… Memory growth enabled for /physical_device:GPU:0
âœ… Using float32 precision (optimized for GTX 1050/Pascal)
   This provides better performance than mixed precision on compute capability 6.1
âœ… GPU configuration complete! GTX 1050 optimized with correct CUDA paths.
âœ… GPU configuration successful - using optimized settings
ðŸš€ ENHANCED TRAINING PIPELINE
======================================================================
ðŸ”§ Normalization: per_sample (strategy: separate)
ðŸ“Š Dataset fraction: 15.0%
ðŸ’¾ Memory limit: 3.5GB
ðŸŽ¯ Validation strategy: auto
ðŸ“Š Memory monitoring: Enabled
ðŸ“ˆ Progress bars: Enabled
ðŸ–¥ï¸  GPU optimization: Enabled

ðŸ“Š Initial system status:
ðŸ’¾ Initial memory usage: 0.44GB

ðŸ“¦ Preparing dataset with smart normalization...
ðŸ§  Memory limit applied: 3.5GB
   ðŸ“Š Calculated chunk size: 5,000 samples
   ðŸ“Š Estimated memory per chunk: 0.11GB
ðŸ” Checking cached dataset: data/preprocessed_dataset.npz
âœ… Cache parameters match - loading cached data
ðŸ“Š Cached data loaded:
   Train: (86016, 6016) | Test: (21504, 6016)
   Range: [-4.826004, 5.000000]
âœ… Dataset ready!
   ðŸ“Š Dataset loaded in 46.1s
   ðŸš€ Train: (86016, 6016), Test: (21504, 6016)
   ðŸ“‰ Using 15.0% of full dataset
   ðŸ“ˆ Total features: 6,016
   ðŸ§  EEG structure: 32 channels Ã— 188 timepoints
   ðŸ’¾ Memory usage: 5.26GB (Î”+4.82GB)
   ðŸ”¢ Data range: [-4.826, 5.000]
   ðŸ“Š Data stats: mean=-0.002794, std=0.984329
   âœ… No extreme outliers detected - normalization looks good!

ðŸš€ Training 1 models with memory leak prevention...
======================================================================

==================== CNN_LSTM (1/1) ====================
ðŸ“Š Memory before cnn_lstm: 5.26GB
ðŸ“Š Dataset size: 86,016 samples
ðŸŽ¯ Using validation strategy: split
ðŸš€ Training cnn_lstm with simple split validation...
ðŸ“Š Memory before cnn_lstm: CPU=5.26GB, GPU=0.09GB
ðŸ”§ Preparing data for cnn_lstm...
   Original data shape: (86016, 6016)
   EEG structure: 32 channels Ã— 188 timepoints = 6016 features
   Reshaped to EEG structure: (86016, 188, 32)
   Input shape for model: (188, 32)
[DEBUG] CNN-LSTM input_shape received: (188, 32)
[INFO] Assuming format: (timesteps=188, features=32)
2025-06-06 08:14:07.796651: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1726] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.
2025-06-06 08:14:07.796755: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node
Your kernel may have been built without NUMA support.
2025-06-06 08:14:07.797340: I tensorflow/core/common_runtime/gpu/gpu_process_state.cc:227] Using CUDA malloc Async allocator for GPU: 0
2025-06-06 08:14:08.087133: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 1875 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1050, pci bus id: 0000:01:00.0, compute capability: 6.1
ðŸ”§ Creating adam optimizer with lr=0.0002
ðŸ“Š Model summary for cnn_lstm:
   Parameters: 9,649
   Input shape: (188, 32)
2025-06-06 08:14:15.694446: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 1862890496 exceeds 10% of free system memory.
2025-06-06 08:16:05.938812: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 1862890496 exceeds 10% of free system memory.
                                                                                                                                           ðŸ§  Small model (9,649 params) - patience reduced to 8                                                            | 0/30 [00:00<?, ?epoch/s]
2025-06-06 08:17:27.546877: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8907
                                                                                                                                           ðŸ›‘ Early stopping: Plateau detected after 8 epochs[21:57<05:16, 52.74s/epoch] , Loss=0.0042, Acc=0.999, Val_Acc=1.000, LR=2.0e-04, GPU=2.3GB
   Best val_accuracy: 1.000000 at epoch 15
ðŸ”„ Restoring best weights
âœ… cnn_lstm (1322.0s):  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 24/30 [21:59<05:29, 54.99s/epoch] , Loss=0.0042, Acc=0.999, Val_Acc=1.000, LR=2.0e-04, GPU=2.3GB
â±ï¸  Training completed in 1453.1sâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 24/30 [21:59<05:16, 52.74s/epoch] , Loss=0.0042, Acc=0.999, Val_Acc=1.000, LR=2.0e-04, GPU=2.3GB
ðŸ“ˆ Training curves saved to: models/cnn_lstm_training_20250606_083829.png
ðŸ” Evaluating on test set...

Evaluation for cnn_lstm
[[10752     0]
 [    0 10752]]
              precision    recall  f1-score   support

         ECG       1.00      1.00      1.00     10752
         EEG       1.00      1.00      1.00     10752

    accuracy                           1.00     21504
   macro avg       1.00      1.00      1.00     21504
weighted avg       1.00      1.00      1.00     21504

ðŸ“Š Memory after cnn_lstm: CPU=7.11GB, GPU=2.17GB
âœ… cnn_lstm model saved
âœ… cnn_lstm completed in 1605.6s
ðŸ§¹ Cleaning up memory...
ðŸ§¹ Cleaning up TensorFlow memory...
âœ… Memory cleanup completed
ðŸ“Š Memory after cleanup: 6.57GB (Î”+1.30GB)

ðŸŽ‰ TRAINING PIPELINE COMPLETE!
======================================================================
â±ï¸  Total time: 26.8 minutes
ðŸ“Š Results summary:
   âœ… Successful: 1/1
   âŒ Failed: 0/1
   ðŸš€ Fastest: cnn_lstm (1605.6s)
ðŸ“Š Final memory usage: 6.57GB (total change: +6.13GB)
