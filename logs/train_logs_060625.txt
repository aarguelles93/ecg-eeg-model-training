(tf-gpu-env) root@DESKTOP-08HJS8G:/mnt/d/Documents/Projects/Thesis/src# python train.py all --dataset-fraction 0.15 --normalization per_sample
ðŸ”§ Environment variables set for TensorFlow/CUDA compatibility
âœ… Import safety check passed - TensorFlow not yet imported
2025-06-06 01:23:58.893071: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-06-06 01:24:07.057461: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
ðŸ”§ Setting up GPU configuration...
ðŸš€ Setting up GPU-friendly configuration for GTX 1050 (2GB VRAM)...
âœ… CUDA libdevice path set to: /usr/lib/cuda
ðŸ”§ Configuring GPU memory settings...
2025-06-06 01:24:26.950701: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node
Your kernel may have been built without NUMA support.
2025-06-06 01:24:28.854869: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node
Your kernel may have been built without NUMA support.
2025-06-06 01:24:28.855066: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node
Your kernel may have been built without NUMA support.
âœ… Found 1 GPU(s): ['/physical_device:GPU:0']
âœ… Memory growth enabled for /physical_device:GPU:0
ðŸ”§ Configuring TensorFlow memory management...
âœ… Memory growth enabled for /physical_device:GPU:0
âœ… Using float32 precision (optimized for GTX 1050/Pascal)
   This provides better performance than mixed precision on compute capability 6.1
âœ… GPU configuration complete! GTX 1050 optimized with correct CUDA paths.
âœ… GPU configuration successful - using optimized settings
ðŸš€ ENHANCED TRAINING PIPELINE
======================================================================
ðŸ”§ Normalization: per_sample (strategy: separate)
ðŸ“Š Dataset fraction: 15.0%
ðŸ’¾ Memory limit: 3.5GB
ðŸŽ¯ Validation strategy: auto
ðŸ“Š Memory monitoring: Enabled
ðŸ“ˆ Progress bars: Enabled
ðŸ–¥ï¸  GPU optimization: Enabled

ðŸ“Š Initial system status:
ðŸ’¾ Initial memory usage: 0.44GB

ðŸ“¦ Preparing dataset with smart normalization...
ðŸ§  Memory limit applied: 3.5GB
   ðŸ“Š Calculated chunk size: 5,000 samples
   ðŸ“Š Estimated memory per chunk: 0.11GB
ðŸ” Checking cached dataset: data/preprocessed_dataset.npz
âœ… Cache parameters match - loading cached data
ðŸ“Š Cached data loaded:
   Train: (86016, 6016) | Test: (21504, 6016)
   Range: [-4.826004, 5.000000]
âœ… Dataset ready!
   ðŸ“Š Dataset loaded in 54.9s
   ðŸš€ Train: (86016, 6016), Test: (21504, 6016)
   ðŸ“‰ Using 15.0% of full dataset
   ðŸ“ˆ Total features: 6,016
   ðŸ§  EEG structure: 32 channels Ã— 188 timepoints
   ðŸ’¾ Memory usage: 5.26GB (Î”+4.82GB)
   ðŸ”¢ Data range: [-4.826, 5.000]
   ðŸ“Š Data stats: mean=-0.002794, std=0.984329
   âœ… No extreme outliers detected - normalization looks good!

ðŸš€ Training 6 models with memory leak prevention...
======================================================================

==================== SVM (1/6) ====================
ðŸ“Š Memory before svm: 5.26GB

ðŸ”§ Training SVM...
ðŸ“Š Memory before SVM: CPU=5.26GB, GPU=0.09GB
ðŸ“Š Building SVM model...
ðŸš€ Training SVM...
âœ… SVM training completed in 356.4s

Evaluation for SVM
[[10752     0]
 [    0 10752]]
              precision    recall  f1-score   support

         ECG       1.00      1.00      1.00     10752
         EEG       1.00      1.00      1.00     10752

    accuracy                           1.00     21504
   macro avg       1.00      1.00      1.00     21504
weighted avg       1.00      1.00      1.00     21504

ðŸ’¾ SVM model saved
ðŸ“Š Memory after SVM: CPU=5.29GB, GPU=0.09GB
âœ… svm completed in 375.9s
ðŸ§¹ Cleaning up memory...
ðŸ§¹ Cleaning up TensorFlow memory...
âœ… Memory cleanup completed
ðŸ“Š Memory after cleanup: 5.28GB (Î”+0.02GB)

==================== SIMPLE_CNN (2/6) ====================
ðŸ“Š Memory before simple_cnn: 5.28GB
ðŸ“Š Dataset size: 86,016 samples
ðŸŽ¯ Using validation strategy: split
ðŸš€ Training simple_cnn with simple split validation...
ðŸ“Š Memory before simple_cnn: CPU=5.28GB, GPU=0.09GB
ðŸ”§ Preparing data for simple_cnn...
   Original data shape: (86016, 6016)
   EEG structure: 32 channels Ã— 188 timepoints = 6016 features
   Reshaped to EEG structure: (86016, 188, 32)
   Input shape for model: (188, 32)
2025-06-06 01:32:00.517057: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node
Your kernel may have been built without NUMA support.
2025-06-06 01:32:00.518187: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node
Your kernel may have been built without NUMA support.
2025-06-06 01:32:00.518304: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node
Your kernel may have been built without NUMA support.
2025-06-06 01:32:03.809932: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node
Your kernel may have been built without NUMA support.
2025-06-06 01:32:03.810559: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node
Your kernel may have been built without NUMA support.
2025-06-06 01:32:03.810816: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1726] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.
2025-06-06 01:32:03.811000: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node
Your kernel may have been built without NUMA support.
2025-06-06 01:32:03.811395: I tensorflow/core/common_runtime/gpu/gpu_process_state.cc:227] Using CUDA malloc Async allocator for GPU: 0
2025-06-06 01:32:03.899139: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 1875 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1050, pci bus id: 0000:01:00.0, compute capability: 6.1
ðŸ”§ Creating adam optimizer with lr=0.0001
ðŸ“Š Model summary for simple_cnn:
   Parameters: 95,409
   Input shape: (188, 32)
2025-06-06 01:32:12.346494: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 1862890496 exceeds 10% of free system memory.
2025-06-06 01:32:31.615751: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 1862890496 exceeds 10% of free system memory.
                                                                                                                 ðŸ§  Small model (95,409 params) - patience reduced to 8                                 | 0/30 [00:00<?, ?epoch/s]
2025-06-06 01:33:05.201148: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8907

Epoch 6: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.1.000, Val_Acc=1.000, LR=1.0e-04, GPU=2

Epoch 11: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.1.000, Val_Acc=1.000, LR=5.0e-05, GPU=

Epoch 16: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.1.000, Val_Acc=1.000, LR=2.5e-05, GPU=

Epoch 21: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-06.=1.000, Val_Acc=1.000, LR=1.2e-05, GPU=
                                                                                                                  ðŸ›‘ Early stopping: Plateau detected after 8 epochs4/30 [13:17<03:07, 31.32s/epoch] , Loss=0.0000, Acc=1.000, Val_Acc=1.000, LR=6.2e-06, GPU=
   Best val_accuracy: 1.000000 at epoch 15
ðŸ”„ Restoring best weights
âœ… simple_cnn (798.9s):  80%|â–Š| 24/30 [13:18<03:19, 33.25s/epoch] , Loss=0.0000, Acc=1.000, Val_Acc=1.000, LR=6.2e
â±ï¸  Training completed in 834.0s24/30 [13:18<03:07, 31.32s/epoch] , Loss=0.0000, Acc=1.000, Val_Acc=1.000, LR=6.2e
ðŸ“ˆ Training curves saved to: models/simple_cnn_training_20250606_014602.png
ðŸ” Evaluating on test set...

Evaluation for simple_cnn
[[10752     0]
 [    0 10752]]
              precision    recall  f1-score   support

         ECG       1.00      1.00      1.00     10752
         EEG       1.00      1.00      1.00     10752

    accuracy                           1.00     21504
   macro avg       1.00      1.00      1.00     21504
weighted avg       1.00      1.00      1.00     21504

ðŸ“Š Memory after simple_cnn: CPU=7.04GB, GPU=2.17GB
âœ… simple_cnn model saved
âœ… simple_cnn completed in 859.5s
ðŸ§¹ Cleaning up memory...
ðŸ§¹ Cleaning up TensorFlow memory...
âœ… Memory cleanup completed
ðŸ“Š Memory after cleanup: 6.51GB (Î”+1.24GB)

==================== CNN_LSTM (3/6) ====================
ðŸ“Š Memory before cnn_lstm: 6.51GB
ðŸ“Š Dataset size: 86,016 samples
ðŸŽ¯ Using validation strategy: split
ðŸš€ Training cnn_lstm with simple split validation...
ðŸ“Š Memory before cnn_lstm: CPU=6.51GB, GPU=2.17GB
ðŸ”§ Preparing data for cnn_lstm...
   Original data shape: (86016, 6016)
   EEG structure: 32 channels Ã— 188 timepoints = 6016 features
   Reshaped to EEG structure: (86016, 188, 32)
   Input shape for model: (188, 32)
[DEBUG] CNN-LSTM input_shape received: (188, 32)
[INFO] Assuming format: (timesteps=188, features=32)
âŒ cnn_lstm failed after 0.1s: list index out of range
ðŸ§¹ Cleaning up memory...
ðŸ§¹ Cleaning up TensorFlow memory...
âœ… Memory cleanup completed
ðŸ“Š Memory after cleanup: 6.51GB (Î”-0.00GB)

==================== MLP (4/6) ====================
ðŸ“Š Memory before mlp: 6.51GB
ðŸ“Š Dataset size: 86,016 samples
ðŸŽ¯ Using validation strategy: split
ðŸš€ Training mlp with simple split validation...
ðŸ“Š Memory before mlp: CPU=6.51GB, GPU=2.17GB
ðŸ”§ Preparing data for mlp...
   Original data shape: (86016, 6016)
   MLP flattened shape: (86016, 6016)
ðŸ”§ Creating adam optimizer with lr=0.0001
ðŸ“Š Model summary for mlp:
   Parameters: 387,201
   Input shape: (6016,)
2025-06-06 01:49:11.900353: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 1862890496 exceeds 10% of free system memory.
2025-06-06 01:54:36.273921: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 1862890496 exceeds 10% of free system memory.

Epoch 6: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.h] , Loss=0.0003, Acc=1.000, Val_Acc=1.000, LR=1.0e-04, GPU=2.4GB

Epoch 11: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.] , Loss=0.0002, Acc=1.000, Val_Acc=1.000, LR=5.0e-05, GPU=2.4GB

Epoch 16: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.] , Loss=0.0001, Acc=1.000, Val_Acc=1.000, LR=2.5e-05, GPU=2.4GB

Epoch 21: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-06.h] , Loss=0.0000, Acc=1.000, Val_Acc=1.000, LR=1.2e-05, GPU=2.4GB

Epoch 26: ReduceLROnPlateau reducing learning rate to 3.12499992105586e-06.h] , Loss=0.0000, Acc=1.000, Val_Acc=1.000, LR=6.2e-06, GPU=2.4GB
                                                                                                                                           ðŸ›‘ Early stopping: Plateau detected after 12 epochs05:57<00:25, 12.55s/epoch] , Loss=0.0001, Acc=1.000, Val_Acc=1.000, LR=3.1e-06, GPU=2.4GB
   Best val_accuracy: 1.000000 at epoch 15
ðŸ”„ Restoring best weights
âœ… mlp (358.0s):  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 28/30 [05:57<00:25, 12.78s/epoch] , Loss=0.0001, Acc=1.000, Val_Acc=1.000, LR=3.1e-06, GPU=2.4GB
â±ï¸  Training completed in 870.3sâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 28/30 [05:57<00:25, 12.55s/epoch] , Loss=0.0001, Acc=1.000, Val_Acc=1.000, LR=3.1e-06, GPU=2.4GB
ðŸ“ˆ Training curves saved to: models/mlp_training_20250606_020051.png
ðŸ” Evaluating on test set...

Evaluation for mlp
[[10751     1]
 [    0 10752]]
              precision    recall  f1-score   support

         ECG       1.00      1.00      1.00     10752
         EEG       1.00      1.00      1.00     10752

    accuracy                           1.00     21504
   macro avg       1.00      1.00      1.00     21504
weighted avg       1.00      1.00      1.00     21504

ðŸ“Š Memory after mlp: CPU=8.93GB, GPU=2.92GB
âœ… mlp model saved
âœ… mlp completed in 888.8s
ðŸ§¹ Cleaning up memory...
ðŸ§¹ Cleaning up TensorFlow memory...
âœ… Memory cleanup completed
ðŸ“Š Memory after cleanup: 6.47GB (Î”-0.04GB)

==================== TCN (5/6) ====================
ðŸ“Š Memory before tcn: 6.47GB
ðŸ“Š Dataset size: 86,016 samples
ðŸŽ¯ Using validation strategy: split
ðŸš€ Training tcn with simple split validation...
ðŸ“Š Memory before tcn: CPU=6.47GB, GPU=2.92GB
ðŸ”§ Preparing data for tcn...
   Original data shape: (86016, 6016)
   EEG structure: 32 channels Ã— 188 timepoints = 6016 features
   Reshaped to EEG structure: (86016, 188, 32)
   Input shape for model: (188, 32)
ðŸ”§ Creating adam optimizer with lr=0.0001
ðŸ“Š Model summary for tcn:
   Parameters: 6,497
   Input shape: (188, 32)
2025-06-06 02:02:55.204918: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 1862890496 exceeds 10% of free system memory.
                                                                                                                                           ðŸ§  Small model (6,497 params) - patience reduced to 8                                                            | 0/30 [00:00<?, ?epoch/s]
2025-06-06 02:08:58.832116: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:306] gpu_async_0 cuMemAllocAsync failed to allocate 456401920 bytes: CUDA error: out of memory (CUDA_ERROR_OUT_OF_MEMORY)
 Reported by CUDA: Free memory/Total memory: 0/3221094400
2025-06-06 02:08:58.878741: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:311] Stats: Limit:                      1966971291
InUse:                      2560859728
MaxInUse:                   2660311672
NumAllocs:                    14215183
MaxAllocSize:               1862890496
Reserved:                            0
PeakReserved:                        0
LargestFreeBlock:                    0

2025-06-06 02:08:58.882099: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:63] Histogram of current allocation: (allocation_size_in_bytes, nb_allocation_of_that_sizes), ...;
2025-06-06 02:08:58.882191: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 4, 55
2025-06-06 02:08:58.882202: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 8, 12
2025-06-06 02:08:58.882208: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 16, 1
2025-06-06 02:08:58.882213: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 96, 32
2025-06-06 02:08:58.882218: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 128, 6
2025-06-06 02:08:58.882224: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 1028, 1
2025-06-06 02:08:58.882251: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 2304, 3
2025-06-06 02:08:58.882278: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 3072, 7
2025-06-06 02:08:58.882288: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 6912, 3
2025-06-06 02:08:58.882293: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 9216, 4
2025-06-06 02:08:58.882298: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 36096, 2
2025-06-06 02:08:58.882304: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 48128, 1
2025-06-06 02:08:58.882330: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 48640, 1
2025-06-06 02:08:58.882338: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 68816, 1
2025-06-06 02:08:58.882343: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 619312, 1
2025-06-06 02:08:58.882348: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 16813312, 2
2025-06-06 02:08:58.882353: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 206998528, 1
2025-06-06 02:08:58.882359: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 456395776, 1
2025-06-06 02:08:58.882364: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 1862890496, 1
2025-06-06 02:08:58.941896: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:97] CU_MEMPOOL_ATTR_RESERVED_MEM_CURRENT: 2684354560
2025-06-06 02:08:58.941957: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:99] CU_MEMPOOL_ATTR_USED_MEM_CURRENT: 2560859728
2025-06-06 02:08:58.941967: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:100] CU_MEMPOOL_ATTR_RESERVED_MEM_HIGH: 3154116608
2025-06-06 02:08:58.941973: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:101] CU_MEMPOOL_ATTR_USED_MEM_HIGH: 2660311672

Epoch 7: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.h] , Loss=0.0000, Acc=1.000, Val_Acc=1.000, LR=1.0e-04, GPU=2.4GB

Epoch 12: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.] , Loss=0.0000, Acc=1.000, Val_Acc=1.000, LR=5.0e-05, GPU=2.4GB

Epoch 17: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.] , Loss=0.0000, Acc=1.000, Val_Acc=1.000, LR=2.5e-05, GPU=2.4GB

Epoch 22: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-06.h] , Loss=0.0000, Acc=1.000, Val_Acc=1.000, LR=1.2e-05, GPU=2.4GB
                                                                                                                                           ðŸ›‘ Early stopping: Plateau detected after 8 epochs[30:39<07:11, 71.83s/epoch] , Loss=0.0000, Acc=1.000, Val_Acc=1.000, LR=6.2e-06, GPU=2.4GB
   Best val_accuracy: 1.000000 at epoch 15
ðŸ”„ Restoring best weights
âœ… tcn (1840.4s):  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 24/30 [30:39<07:39, 76.66s/epoch] , Loss=0.0000, Acc=1.000, Val_Acc=1.000, LR=6.2e-06, GPU=2.4GB
â±ï¸  Training completed in 2107.7sâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 24/30 [30:39<07:11, 71.83s/epoch] , Loss=0.0000, Acc=1.000, Val_Acc=1.000, LR=6.2e-06, GPU=2.4GB
ðŸ“ˆ Training curves saved to: models/tcn_training_20250606_023629.png
ðŸ” Evaluating on test set...
2025-06-06 02:36:52.845605: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:306] gpu_async_0 cuMemAllocAsync failed to allocate 16778368 bytes: CUDA error: out of memory (CUDA_ERROR_OUT_OF_MEMORY)
 Reported by CUDA: Free memory/Total memory: 0/3221094400
2025-06-06 02:36:52.845688: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:311] Stats: Limit:                      1966971291
InUse:                      2642331084
MaxInUse:                   2660311672
NumAllocs:                    34842687
MaxAllocSize:               1862890496
Reserved:                            0
PeakReserved:                        0
LargestFreeBlock:                    0

2025-06-06 02:36:52.845711: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:63] Histogram of current allocation: (allocation_size_in_bytes, nb_allocation_of_that_sizes), ...;
2025-06-06 02:36:52.845719: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 4, 54
2025-06-06 02:36:52.845725: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 8, 12
2025-06-06 02:36:52.845730: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 16, 1
2025-06-06 02:36:52.845745: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 96, 32
2025-06-06 02:36:52.845758: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 128, 6
2025-06-06 02:36:52.845768: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 1028, 1
2025-06-06 02:36:52.845776: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 2304, 3
2025-06-06 02:36:52.845784: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 3072, 7
2025-06-06 02:36:52.845792: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 6912, 3
2025-06-06 02:36:52.845799: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 9216, 4
2025-06-06 02:36:52.845807: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 68816, 1
2025-06-06 02:36:52.845814: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 577536, 2
2025-06-06 02:36:52.845822: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 619312, 1
2025-06-06 02:36:52.845829: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 770048, 1
2025-06-06 02:36:52.845836: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 778240, 1
2025-06-06 02:36:52.845842: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 16777600, 1
2025-06-06 02:36:52.845849: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 17354752, 2
2025-06-06 02:36:52.845856: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 206998528, 1
2025-06-06 02:36:52.845862: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 517472256, 1
2025-06-06 02:36:52.845869: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 1862890496, 1
2025-06-06 02:36:52.845878: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:97] CU_MEMPOOL_ATTR_RESERVED_MEM_CURRENT: 2785017856
2025-06-06 02:36:52.845884: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:99] CU_MEMPOOL_ATTR_USED_MEM_CURRENT: 2642331084
2025-06-06 02:36:52.845889: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:100] CU_MEMPOOL_ATTR_RESERVED_MEM_HIGH: 3154116608
2025-06-06 02:36:52.845895: E tensorflow/compiler/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:101] CU_MEMPOOL_ATTR_USED_MEM_HIGH: 2660311672

Evaluation for tcn
[[10752     0]
 [    0 10752]]
              precision    recall  f1-score   support

         ECG       1.00      1.00      1.00     10752
         EEG       1.00      1.00      1.00     10752

    accuracy                           1.00     21504
   macro avg       1.00      1.00      1.00     21504
weighted avg       1.00      1.00      1.00     21504

ðŸ“Š Memory after tcn: CPU=6.66GB, GPU=2.17GB
âœ… tcn model saved
âœ… tcn completed in 2143.4s
ðŸ§¹ Cleaning up memory...
ðŸ§¹ Cleaning up TensorFlow memory...
âœ… Memory cleanup completed
ðŸ“Š Memory after cleanup: 6.13GB (Î”-0.35GB)

==================== DUAL_BRANCH (6/6) ====================
ðŸ“Š Memory before dual_branch: 6.13GB

ðŸš€ Training Dual-Branch CNN...
ðŸ“Š Memory before dual_branch: CPU=6.13GB, GPU=2.17GB
ðŸ”§ Preparing data for dual_branch...
   Original data shape: (86016, 6016)
   EEG structure: 32 channels Ã— 188 timepoints = 6016 features
   Reshaped to EEG structure: (86016, 188, 32)
   Input shape for model: (188, 32)
ðŸ“Š Dataset size: 86,016 samples
ðŸŽ¯ Using validation strategy: split
ðŸ”§ Creating adam optimizer with lr=0.0001
ðŸ“Š Model summary for dual_branch:
   Parameters: 14,721
   Input shape: (188, 32)
ðŸ“ˆ dual_branch:   0%|                                                                                            | 0/30 [00:00<?, ?epoch/s] ðŸ§  Small model (14,721 params) - patience reduced to 8
ðŸ“ˆ dual_branch:   3%|â–‹                  | 1/30 [03:22<1:37:40, 202.10s/epoch] , Loss=0.2114, Acc=0.984, Val_Acc=1.000, LR=1.0e-04, GPU=2.4GB2025-06-06 02:42:15.575566: W tensorflow/core/framework/op_kernel.cc:1828] OP_REQUIRES failed at save_restore_v2_ops.cc:160 : PERMISSION_DENIED: models/dual_branch_best/variables/variables_temp/part-00000-of-00001.data-00000-of-00001.tempstate5266047210793380547; Permission denied
âŒ dual_branch failed after 311.4s: {{function_node __wrapped__SaveV2_dtypes_68_device_/job:localhost/replica:0/task:0/device:CPU:0}} models/dual_branch_best/variables/variables_temp/part-00000-of-00001.data-00000-of-00001.tempstate5266047210793380547; Permission denied [Op:SaveV2]
ðŸ“ˆ dual_branch:   3%|â–‹                  | 1/30 [03:32<1:42:40, 212.45s/epoch] , Loss=0.2114, Acc=0.984, Val_Acc=1.000, LR=1.0e-04, GPU=2.4GB
ðŸ§¹ Cleaning up memory...
ðŸ§¹ Cleaning up TensorFlow memory...
âœ… Memory cleanup completed
ðŸ“Š Memory after cleanup: 6.08GB (Î”-0.04GB)

ðŸŽ‰ TRAINING PIPELINE COMPLETE!
======================================================================
â±ï¸  Total time: 76.6 minutes
ðŸ“Š Results summary:
   âœ… Successful: 4/6
   âŒ Failed: 2/6
   ðŸš€ Fastest: cnn_lstm (0.1s)
   ðŸ’¥ Failed models: cnn_lstm, dual_branch
ðŸ“Š Final memory usage: 6.08GB (total change: +5.64GB)

ðŸ’¡ Next steps:
   â€¢ Check model performance in the models/ directory
   â€¢ Compare training curves to identify best performing models
   â€¢ Use the best performing model for inference